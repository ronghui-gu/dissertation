
\chapter{Evaluation and Lessons Learned}
\label{chap-eval}
To evaluate
our CertiKOS framework
and a series of verified kernels,
we have analyzed the performance of the \mCTOShyper{} hypervisor kernel and the  \cCTOS{} concurrent kernel
with a thorough experimental benchmark evaluation.
Furthermore, an extended version of \mCTOShyper{}
was deployed in a practical system that is used in the context of a
large DARPA-funded research project. Our experiments with benchmarks
confirm the observations made during deployment: the performance
overhead of \mCTOShyper{} is moderate. We are convinced that it is
practical to use our verification framework to produce competitive
real-world kernels with acceptable effort.

\ronghui{Fix the road map}

\section{Performance Evaluation} 
Although the performance is not the main emphasis of this paper, we
have run a number of micro and macro benchmarks to measure the
overhead of {\mCTOS} and {\cCTOS}
to compare them to existing systems such
as KVM and seL4. All experiments have been performed on an Intel Core
i7-2600 S (4 cores / 8 threads) with 8 MB L3 cache, 16 GB memory, and
a 120 GB Intel 520 SSD. Since the power control code has not been
verified, we disabled the turbo boost and power management features of
the hardware during experiments.
A comparison of the performance of seL4 and \cCTOS{} is not
straightforward since the verified \cCTOS{} kernels run on 
multicore
x86
platforms but the verified seL4 runs on ARMv6
and ARMv7 hardware and only supports single-core. Moreover, the
verified version of seL4 does not have virtualization support and
cannot boot Linux. As a result, we do not compare hypervisor
performance but instead focus on a comparison of the IPC performance
of {\cCTOS} and an \emph{unverified} x86 version of seL4.


\paragraph{IPC Performance}
We use seL4's IPC benchmark
sel4bench-manifest\footnote{https://github.com/smaccm/sel4bench-manifest}
with processes in different address spaces and with identical
scheduler priorities, both in \emph{slowpath} and \emph{fastpath}
configurations. To run this benchmark on {\cCTOS}, we replaced seL4's
\emph{Call} and \emph{ReplyWait} system calls with \cCTOS's
synchronous \emph{send} and \emph{receive} calls.  On {\cCTOS}, we
measure latency of IPC in both settings when the sender and receiver
are running on the same CPU or across different CPUs.
Figure~\ref{fig:eval_ipc} contains a compilation of our results. It
shows the average number of clock cycles needed for the operations for
message sizes of 0 and 10. The IPC system calls in seL4 and {\cCTOS}
are quite different, and they are running in different machine
settings (single- vs. multicore), which makes the precise
comparison very complicated.  Still, as shown in the figure, IPC
performance of {\cCTOS} is relatively slower than that of seL4. The
gap is not fundamental to our verification approach.  Because seL4
follows the microkernel design philosophy, its IPC performance is
critical. IPC implementations in seL4 are highly optimized, and
heavily tailored to specific hardware platforms.
Verification not only
should not hinder application of similar performance optimizations,
but instead provide a safety net for more aggressive optimizations, if
it is required for application scenarios of the kernel we have in
mind.

On {\cCTOS}, the performance of inter-core IPC is worse than that of
intra-core IPC.  The reason is that, after the receiver thread is
woken up, it is first pushed onto the \emph{pending queue} of the
corresponding CPU, which gets pushed back to the scheduler's ready
queue at the subsequent scheduling point. Furthermore, as shown in the
figure, the performance of the send call is not affected by the buffer
size. That is because the send system call simply saves the virtual
address and size of the user buffer into the channel.  The actual
copying of data is all done by the receive call.  Before copying the
data, the receive call needs to switch the page map to an identity
map, copy the data, and switch the page map back before it goes back
to user. Note that switching page map causes TLB flushes, which
imposes extra performance overhead to the receive call in addition to
the overhead of copying the data.

\begin{figure}\centering
		\includegraphics[width=10cm]{figs/ipc.pdf}
		\caption{IPC performance between \cCTOS{} and  seL4 x86.}
		\label{fig:eval_ipc}
		\hrulefill
\end{figure}


\paragraph{Spinlock Performance}
\begin{figure}\centering
		\includegraphics[width=12cm]{figs/locks.pdf}
		\caption{Latency and throughput of ticket and MCS lock in \cCTOS{}.}
		\label{fig:eval_spin}
\end{figure}

Figure~\ref{fig:eval_spin} shows the latency and throughput of the two
spinlock implementation in \cCTOS. The latency is measured when an
empty critical section (payload) is executed by the protection of the
same lock. We take a sample of 10,000 consecutive lock acquiring and
releasing as one round.  The throughput is measured by the
number of the payload executed per millisecond. Without locking, the
throughput of the payload is 73,500 times per millisecond.  As shown
in Figure~\ref{fig:eval_spin}, MCS lock performs much worse than the
ticket lock on a single-core or a dual-core machine, but its
performance gets better as the number of cores increases.


\paragraph{Hypervisor Performance} 
\begin{figure}\centering
		\includegraphics[width=12cm]{figs/hyp_macro.pdf}
		\caption{Normalized macro benchmarks: Linux on KVM and \mCTOShyper{}, baseline is Linux on bare metal}
		\label{fig:eval_macro}
		\hrulefill
\end{figure}

To evaluate \mCTOShyper{} as a hypervisor, we measure the performance of
some macro benchmarks on Ubuntu 12.04.2 LTS running as a guest.  We ran the
benchmarks on Linux as guest in KVM and \mCTOShyper{}, as well as on the bare metal.
Figure~\ref{fig:eval_macro} contains a compilation of standard macro
benchmarks: unpacking of the Linux 4.0-rc4 kernel, compilation of the
Linux 4.0-rc4 kernel, Apache HTTPerf~\cite{mosberger1998} (running on
loopback), and DaCaPo Benchmark 9.12~\cite{dacapo2006}.

\ignore{
To begin with a sanity check, McVoy's LMbench (version 3-alpha1) is used to
measure the micro-level metrics of operating system and hardware, which includes
the bandwidths and latency of several file system, local communication systems,
virtual memory, context switch and, basic arithmetic operations. The results
show that 34 out of 60 benchmarks of \mCTOShyper{} are very close to the bare
metal (within the range of 5\%), and the others are listed in the Table . These
results shows that there is no significant overhead of \mCTOShyper{}.
}

In Figure~\ref{fig:eval_macro} we normalize the running time of the benchmarks
using the bare metal performance as the baseline (100\%). The overhead of
\mCTOShyper{} is moderate and comparable to KVM. In some cases, \mCTOShyper{}
performs better than KVM, which is mainly because KVM has a Linux host, and thus has a
larger cache footprint. In other cases, which involve a large amount of file
operations, such as decompressing Linux source and Tomcat server, \mCTOShyper{}
performs worse. This is due to idiosyncrasies of the VirtIO interface. In
\mCTOShyper{}, we expose the raw disk interface to the guest via VirtIO; when
there is a disk I/O request in the \texttt{vring}, \mCTOShyper{} immediately
pulls it and issues a disk command to handle that request and polls until the
command finishes. This leads problems with the guest linux VirtIo driver, since,
when the guest Linux uses VirtIO, it always sends disk I/O request of length
4KB.

Figure~\ref{fig:eval_disk} (b) shows the throughput of the SSD used in our
benchmark against the size of data for each command when writes a large (1GB)
sequential block. This figure indicates that the disk can only reach the top
rate (read 270MB/s, write 90MB/s) when the data size for each command is at
least 8KB for reads and 512KB for writes.
Thus, disk access becomes a bottleneck.
The disk dump benchmark in Figure~\ref{fig:eval_disk} (a)
further demonstrates this issue.

\begin{figure}\centering
		\includegraphics[width=14cm]{figs/disk.pdf}
		\caption{Disk performance: (a). \texttt{lmdd} of vary size files; (b). the SSD throughput used in the benchmarks}
		\label{fig:eval_disk}
		\hrulefill
\end{figure}


The VM monitor in \mCTOShyper{} is running as a user
process in the ring 3 mode. This leads to a smaller kernel
size and makes it possible to use emulated devices and unverified
drivers at the user level.  The downside is that each
VM entry and exit causes an additional ring switch, and VM-related
information must be copied to the monitor process in order for it to
process the exit. Therefore, it may have an impact on performance,
especially for those guest programs that frequently cause VM exits,
such as web servers, which generate frequent network-related external
interrupts.  Another approach is to verify the virtual machine monitor
code and run it inside a kernel module, \eg, in a ring 0 process.




\input{impl}

\section{Execution Model and Completeness}
The majority of the {\mCTOS} and \cCTOS{} kernels are implemented and verified 
at the C level, and then compiled by a modified version of the CompCert verified
compiler~\cite{dscal15}.  The entire kernel (both C and assembly)
source code, together with the source code for the verified compiler,
are extracted into an OCaml program through Coq's extraction
mechanism. When this OCaml program is executed, the extracted C source code 
is compiled into assembly; the resulting assembly code is then merged 
with the existing assembly kernel source code to produce a single piece 
of assembly code corresponding to our verified kernel.  Thus, our deliverable 
consists of a piece of assembly code for the entire verified kernel, a 
high-level deep specification of various kernel behaviors, and a 
machine-checkable proof object stating that the assembly code running on 
the actual hardware satisfies the high-level specification.

The verified assembly code is then linked with the rest of the kernel code
(the bootloader and remaining unverified drivers) to produce the
actual binary image of the OS. The resulting kernels are practical.
\ignore{it
runs on stock x86 hardware and can successfully boot a guest version
of Linux.}

\ignore{\section{Proof effort}
We take the mCertiKOS kernel presented by Gu {\it et al}
\cite{dscal15}, and extend the kernel with various features such as
dynamic memory management, container support for controlling resource
consumption, Intel hardware virtualization support, shared memory IPC,
single-copy synchronous IPC, ticket and MCS lock implementation, new
schedulers, condition variables, FIFO blocking bounded queues, {\it
  etc}. Note that all of these new features are implemented in the
context of a concurrent machine, whereas the mCertiKOS presented by Gu
{\it et al} \cite{dscal15} only runs on a single-core machine. 

\ignore{ We
have also merged the work by Chen {\it et al} \cite{chen16} on the
interruptible kernel with device drivers using our multicore model.
}

Overall, we have contributed 3,500 additional lines of C and assembly
source code to the mCertiKOS code base. Regarding specification, there
are 943 lines of code used to specify the lowest layer axiomatizing
the hardware machine model, and 450 lines of code for the
specification of the abstract system call interfaces. These are in our
TCB and need to be trusted. We keep these specifications small to
limit the room for errors and ease the review process.  Outside the
TCB, there are 5249 lines of additional specifications for the various
kernel functions, and about 40K lines of code used to define auxiliary
definitions, lemmas, theorems, and invariants. Additionally, there are
50K lines of Coq proof scripts for proving the newly-added kernel
features. At least one third of these auxiliary definitions and proof
scripts are redundant and semi-automatically generated,\ignore{ (or
  copied and pasted),} which makes our proof a little verbose.  For
example, many invariant proofs get duplicated across the layers
whenever there is a minor change to the entire set of invariants.  We
are currently working on a new layer calculus to minimize redundant
definitions and proofs.

On the verification framework side, we developed a general linking
theorem for composing multiple threads running on the same CPU, as
well as a combining programs running on
different CPUs (10K lines).
Our team completed the verification of the new concurrent framework
and the additional features in about 2 person years. 
\ignore{It took 6 person
months to extend mCertiKOS with dynamic memory management, container
support, shared memory and synchronous IPC, and Intel
virtualization. The majority of the time (1.5 person years) was spent
on developing the framework to reason about the concurrent {\mCTOS}
kernel.} If we assume we work 250
days a year, we have written roughly 400 lines of code per
day\ignore{(50 lines per hour)}.  Thanks to the extensive layering,
the proofs for each small component are relatively easy; often it
takes less than two hours to write 400 lines of Coq code.  One of the
most challenging tasks is to come up with the correct specification
and invariants. In our experience, people make many mistakes when
initially writing down the specification; during the code verification
and refinement proof, we frequently find flaws in the specification or
notice that our invariants are too weak.  Furthermore, verification of
some components can be inherently complex, \eg, the kernel
initialization and locks.  It took us 3 days to certify the
ticket-lock acquire function (less than 10 lines of C code), and
multiple person weeks to prove that the ticket-lock is
starvation free.


Furthermore, note that we did not reach the current working solution
in one shot. We first spent about 3 person months developing an
unsuccessful version of the framework for composing multi-threaded
execution on a single CPU.  In that version, thread-local execution
was modeled using a \emph{time stamp} index into a global system
log. We eventually realized that the exact time stamps were too
cumbersome and revealed too much information about the underlying
implementation (\eg, the number of software yields within a function
body), so we spent another month developing a new system that uses
local logs (lists of events) instead.\ignore{ of time stamps, and the
  ability to shuffle and merge the events in the local logs to hide
  unnecessary nondeterminism or implementation details.}  Our initial
multicore machine model also did not work out very well when we
developed the multicore linking framework; we spent 3 person weeks to
improve the initial design through multiple iterations.  The main
challenge was finding the right invariants for the environment
context, such that we could successfully establish starvation-freedom.
}

\ignore{
\paragraph{Abstraction Layers}
\newman{I am gonna write some summary about the layered approach after I read
the overview section to see how much of the concepts are already covered.}
\ronghui{Maybe it is not helpful to include this part in the submission}
\begin{itemize}
\item Abstraction of data representation: doubly linked list --> logical list
\item Stronger invariants
\item Abstraction of primitive specification: hiding log implementation by linking and merging events
\item Refinement of machine models: from a realistic machine model to an ideal machine model that is suitable for our reasoning purpose, e.g., change in machine/memory/interrupt model
\end{itemize}
}

\ignore{
The verification effort roughly falls into three categories: layer
design with specification and invariants, refinement proofs between
the layers, and verification of C and assembly code with respect to
the specifications. The time needed for each of the categories depends
largely on the layer.  For instance, at the boundary of physical and
virtual memory management ({\code{MPTIntro}}), almost all effort
is in the refinement proof, due to the proof for the refinement between
two completely different memory models. More effort went into the
refinement proof when we introduced the Intel \emph{virtual machine
memory model}, where we proved the refinement between the concrete
four level extended page table structure in memory and the abstract
mapping from the guest addresses to the host addresses.
In contrast, for the layer {\code{MATOp}},
which initializes physical memory allocation,
most of the time was spent on verifying
the non-trivial nested loops present in the C code,
while the refinement proofs were derived automatically. 

The proofs were facilitated by automation tools for C
code, layer design patterns, and tactics libraries developed in
recent years \cite{dscal15}. These tools have greatly
reduced the amount of work needed to verify extensions of the kernel.
}


\ignore{\section{Extension and adaptation}
We augmented \mCTOSbase{} to support the hardware-assisted
virtualization technology Intel VT-x, and built a certified hypervisor
\mCTOShyper{}.  We have also built certified kernels with ring-0
process supports.  More details will be provided in the full paper.


First, we augmented \mCTOSbase{} to support the hardware-assisted
virtualization technology Intel VT-x, and built a
certified hypervisor \mCTOShyper{}. Second, we extended \mCTOSbase{}
into \mCTOSringz{} by adding support for
running certifiably-safe programs inside an ``in-kernel
process'' that runs in the privileged ring 0 mode.
We have also built \mCTOSembed{} kernel that is suitable for embedded systems,
by removing the virtual machine management and the virtual
memory management.
Removing plug-ins or layers are achieved simply by 
altering the contextual refinement proof 
at the boundary so that we can glue them back together.
}



%Bugs were found in the original design of $mCertiKOS$ during
%the C verification, which were not revealed during the original testing
%phase. They are mostly related to the overflow or the carelessness
%in implementation (e.g., writing \verb+x & y == 1+ instead of \verb+(x & y) == 1+).
% (Maybe talk about the necessity to have a separate initialization
%layer above intro layers before we introduce the layer implementing
%the actual operations???)
%Despite the frequent changes to the design, we found that the cost of change
%in our layered approach is quite small. (To compare with seL4, maybe mention
%adding new kernel modules, especially adding new kernel data structures to our
%layered design does not require significant changes to the original implementation
%of layers and the proofs.)


   % (a) Did the kernel actually run? If yes, what is the performance like?
   %     If the performance is not great, can we build a new version of
   %     our mCertiKOS kernel so that dramatically improves the performance?
   %     (note this optimized version of mCertiKOS does not have to be
   %      certified; we can explain what needs to be done in order to get
   %      this version certified and leave the actual verification to future
   %      work).

   % (b) Explain how it can manage to boot Linux; what are the key components
   %     and features in our current version of mCertiKOS that made this
   %     possible.

   % (c) Bootloader not verified, but see work by Cai et al;
   %     Stack-usage not verified, but see work by Carbonneaux et al;
   %     Interrupt handlers are ongoing work, see work by Feng et al and
   %     Guo et al. Device derivers not verified.



\ignore{
%\sectskip
\subsection{Extension and Adaptation}
\label{ssec:adapt}
%\asectskip

One primary advantage of our extensible architecture is that it makes
certified kernel extension and reasoning much easier and more principled. 
In this section, we first describe three alternative \mCTOSbase{} kernels
that we created through relatively minor changes to the base kernel. We
then present a specific example of global reasoning over the \mCTOSbase{} 
kernel~--- a simple notion of address space isolation that will serve as 
a starting point for a full-fledged security proof in the future.

We augmented \mCTOSbase{} to support the two hardware-assisted
virtualization technologies Intel VT-x and AMD SVM, and built a
certified hypervisor \mCTOShyper{}.

Fig. \ref{fig:base:vm:layers} shows the 7 layers of the virtual
machine management of \mCTOShyper{} on the Intel platform.
\code{VMInfo} is the layer object
that axiomatizes some of the hardware specific features needed
for the virtualization support. 
Since it is orthogonal to memory and process management,
the \code{VMInfo} object can be horizontally composed with the layers 
below \code{PProc} in \mCTOSbase{}.
On top of this extended \code{PProc} layer,
the virtual machine management extends the \emph{abstract memory model}
with the notions of Extended Page Table (EPT), the virtual machine
control structure (VMCS), and the virtual machine extension meta data (VMX),
which are abstracted into corresponding layer objects.
These objects are again orthogonal to the trap module above and can be
horizontally composed to export related system calls
with minimal cost.
 
\begin{figure}
\includegraphics[scale=0.33]{figs/intel_layer}	
%\vspace*{-14pt}
\caption{Layers of virtual machine management}
\label{fig:base:vm:layers}
%\vspace*{-14pt}
\end{figure}

Thanks to the contextual refinement relation we have proved for
\mCTOSbase{}, one can certify user programs using our formal
specifications of system calls. This gives end-to-end proofs on
the behaviors of user programs when they run on \mCTOSbase{}.  
Furthermore, once certified, these processes can safely run in
the privileged ring 0 mode.  We extended \mCTOSbase{} into
\mCTOSringz{} by adding support for spawning ``in-kernel
processes'' that run in the privileged ring 0 mode. 
Ring 0 processes get much
better system call performance by directly calling kernel
functions and avoiding ring switch and interrupt processing. 

The \mCTOSembed{} kernel is intended for embedded settings. To develop
this kernel we started with \mCTOSringz{} and removed the virtual
machine management, the virtual memory management, and some of the
process management layers that are related to user contexts and user
process management.  Thus \mCTOSembed{} only supports ring 0 processes
which run directly inside the physical kernel address space instead of
the user-level paged virtual address space.

Removing plug-ins or layers does not take much effort.
We only need to alter the contextual refinement proof 
at the boundary so we can glue them back together.

\paragraph{Isolation in \mCTOSbase{}}
\label{security}
We have begun exploring the verification of a global security property
on top of \mCTOSbase{}. As a starting point, we proved a basic notion
of isolation between user-level processes running in different virtual
address spaces. This isolation property is composed of two theorems:
one regarding integrity (write protection), and another regarding
confidentiality (read protection, or noninterference).  The statements
of these two theorems are as follows: suppose the top layer abstract
machine takes one step, changing the machine state from $S$ to $S'$,
and let $p$ be the id of the currently-running process (which can be
found in $S$).
\begin{description}
  \item[Integrity:]
If the value at some non-kernel memory location $l$ differs between
$S$ and $S'$, then $l$ belongs to a page that is mapped in the 
virtual address space of $p$.
\item[Confidentiality:]
\label{confidential}
If the step taken
is not a primitive call to an IPC syscall (send, recv, etc.), then the values
of memory in any address space other than $p$'s cannot have an effect on the
result of the step. In other words, if we altered $S$ 
by changing data in a different process's address space, the step would still 
have the same effect on $p$'s address space.
\end{description}

In the future, we plan to provide a more detailed security policy by
describing what can happen to confidentiality when IPC is used.  This
description will be expressed in terms of propagation of security
labels on the IPC data. Note, however, that our framework allows for
security labels to be specified at a purely logical level~--- there is
no need for concrete representation and manipulation of labels at run
time.

Noninterference properties are generally not preserved across
refinement due to nondeterminism. It may therefore seem that the
aforementioned \emph{confidentiality} holds only at the topmost layer,
but not at lower layers. It turns out, however, that our notion of
deep specification is strong enough to preserve
noninterference. Essentially, to give a deep specification to a
nondeterministic semantics, we must first externalize the source of
nondeterminism (e.g., into an oracle). The noninterference property
then becomes parameterized over this source of nondeterminism, which
allows the parameterized property to be preserved across
refinement. This relationship between deep specification,
noninterference, and refinement will be explored comprehensively in
future work.
}

