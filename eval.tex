
\chapter{Evaluation and Lessons Learned}
\label{chap-eval}
To evaluate
our CertiKOS framework
and a series of verified kernels,
we have analyzed the performance  the  \cCTOS{} concurrent kernel
with a thorough experimental benchmark evaluation.
Furthermore, an extended version of \cCTOS{}
was deployed in a practical system that is used in the context of a
large DARPA-funded research project. 
Section~\ref{sec:eval:perform} shows our experiments with benchmarks, which
confirm the observations made during deployment: the performance
overhead of \cCTOS{} is moderate. 
In Section~\ref{sec:eval:impl},
we presents the implementation and deployment of
our verified kernels.
All in all, we are convinced that it is
practical to use our verification framework to produce competitive
real-world kernels with acceptable effort.


\section{Performance Evaluation} 
\label{sec:eval:perform}
Although the performance is not the main emphasis of this thesis, we
have run a number of micro and macro benchmarks to measure the
overhead of {\cCTOS}
to compare it to existing systems such
as KVM and seL4. All experiments have been performed on an Intel Core
i7-2600 S (4 cores / 8 threads) with 8 MB L3 cache, 16 GB memory, and
a 120 GB Intel 520 SSD. Since the power control code has not been
verified, we disabled the turbo boost and power management features of
the hardware during experiments.
\ignore{A comparison of the performance of seL4 and \cCTOS{} is not
straightforward since the verified \cCTOS{} kernels run on 
multicore
x86
platforms but the verified seL4 runs on ARMv6
and ARMv7 hardware and only supports single-core. Moreover, the
verified version of seL4 does not have virtualization support and
cannot boot Linux. As a result, we do not compare hypervisor
performance but instead focus on a comparison of the IPC performance
of {\cCTOS} and an \emph{unverified} x86 version of seL4.}


\paragraph{Concurrency overhead} The run-time overhead introduced by concurrency
in {\cCTOS} mainly comes from \emph{the latency of spinlocks} and \emph{the
	contention of the shared data}. 

The {\cCTOS} kernel provides two kinds of spinlocks: ticket lock and MCS lock. They
have the same interface and thus are interchangeable. In order to measure
their performance, we put an empty critical section (payload) under the
protection of a single lock. The latency is measured by taking a sample of 10,000
consecutive lock acquires and releases (transactions) on each round. The
throughput is measured by the number of transactions per millisecond.

\begin{figure}\centering
	\hspace{-.2cm}
	\includegraphics[width=15cm]{figs/locks.pdf}
	\hspace{-.2cm}
	\caption{The comparison between slowdown of ticket lock and MCS lock implementations in \cCTOS{}}
	\label{fig:locks}
	\hrulefill
\end{figure}

Figure~\ref{fig:locks} (a) shows the results of latency measurement. In the
single core case, both of these locks impose about 100 cycles of overhead (line chart). 
As the number of cores grows, the latency increases rapidly. However, note
that all transactions are protected by the same lock. Thus, it is expected that
the slowdown should be proportional to the number of cores. In order to show the
slowdown caused by cache contention, we normalized the latency against the
baseline (single core) multiplied by the number of cores. As can be seen from 
the bar chart, the slowdown caused only by cache
contention remains the same for MCS lock, but increases for ticket lock.

Figure~\ref{fig:locks} (b) shows the results of throughput measurement.
Note that without locking, the baseline throughput is 73,500 transactions per millisecond
(not shown in the figure), which is
2.3x and 3.7x of the throughput in the single core case for ticket lock and MCS lock,
respectively. With addition of another core, the throughput drops dramatically
due to cache contention, and then it changes slowly when more cores are added.
The slowdown rate of MCS lock is better than that of ticket lock.

Now that we have compared MCS lock with ticket lock, we will present the
remaining evaluations in this section using only the ticket lock implementation
of \cCTOS{}.

To reduce potential contention, all shared data
structures in {\cCTOS} are carefully designed and pre-allocated with a
fine-grained lock. For example, the process scheduler on each CPU can
avoid accessing any shared data while scheduling, assuming no relevant process is sleeping.
We design a macro-benchmark with server/client pairs to
evaluate the speedup of the system when more cores are introduced. We measure the
number of transactions a server makes per millisecond as the throughput. For each
transaction, the server first performs an IPC receive from channel $i$, 
executes a payload (certain number of `\texttt{nop}' instructions), and finally
sends a message to channel $i + 1$. Correspondingly, the client sends an
IPC message to channel $i$, executes a payload, and then receives the
server's message through channel $i + 1$.

\begin{figure}\centering
	\hspace{-.2cm}
	\includegraphics[width=10cm]{figs/speedup_big_lock.pdf}
	\hspace{-.2cm}
	\caption{Speedup of throughput of \cCTOS{} vs. {\cCTOS-bl} in client/server macro-benchmark on payload (0-2,000) of the server}
	\label{fig:speedup_big_lock}
	\hrulefill
\end{figure}


Figure~\ref{fig:speedup_big_lock} shows this server/client benchmark, comparing {\cCTOS} against a
big-kernel-lock version of {\cCTOS} ({\cCTOS-bl}). Because removing all the
fine-grained locks in the extracted code would require changing many proofs, we simply
insert a lock acquire and release at the top-most layer by hand, without
removing the fine-grained locks in {\cCTOS}. This does not introduce bias
because the speedup is normalized against its own baseline (single core throughput)
for each kernel version separately.
From the figure, we can see that
the speedup rate for big-kernel-lock remains near 1.2x, even as the
number of cores increases. On the other hand, the fine-grained locks of {\cCTOS}
yield significant speedup as the number of cores increases (roughly
1.7x and 2.4x higher with 2 and 3 cores, respectively).

\ignore{
\begin{figure}\centering
	\hspace{-.2cm}
	\includegraphics[width=6cm]{figs/speedup_pb.pdf}
	\hspace{-.2cm}
	\caption{Speedup of the throughput of \cCTOS{} in 2 and 3 cores against various server payloads (0 - 10,000) and IPC message sizes (0 - 20).}
	\label{fig:speedup_pb}
\end{figure}

Figure~\ref{fig:speedup_pb} shows the speedup of {\cCTOS} against various
server payloads and IPC message sizes. Figure~\ref{fig:speedup_pb} (a) shows that
the throughput decreases along with the increase of the server
workload (from I/O intensive task with payload being 0 to computation intensive task with
payload being 10,000) in the line chart. However, the speedup (bar chart) is increasing,
ranging from 1.7x to 1.9x for 2 cores, and 2.3x to 2.7x for 3 cores. The reason
is that the execution of the payload does not require the access to any shared
data, thus the contention becomes less intense. In
Figure~\ref{fig:speedup_pb} (b), the throughput decreases slightly when the
IPC message size increases, but the speedup is not affected.
}

\paragraph{IPC Performance} We measure the latency of IPC send/recv in {\cCTOS}
against various message sizes, and compare the result with seL4's IPC
implementation.

A comparison of the performance of seL4 and \cCTOS{} is not straightforward
since the verified \cCTOS{} kernel runs on a multicore x86 platform while the
verified seL4 kernel runs on ARMv6 and ARMv7 hardware and only supports single-core.
Thus, we use an unverified, single-core version of seL4 for comparison.
Moreover, the synchronized IPC API in seL4 (\texttt{Call/ReplyWait}) has a
different semantics than \cCTOS{}'s send/recv: it uses a round-trip message passing 
protocol over a single channel while trapping into the kernel twice, and it does
not use any standard sleep or wake up procedures. Nevertheless, we
use $\frac{(send + recv) \times 2}{(Call + ReplyWait)}$ to calculate the IPC
performance ratio.

We measure seL4's performance using seL4's IPC benchmark
sel4bench-manifest~\cite{sel4bench} with
processes in different address spaces and with identical scheduler priorities,
both in \emph{slowpath} and \emph{fastpath} configurations. To measure \cCTOS{}'s
performance, we simply replace seL4's
\emph{Call} and \emph{ReplyWait} system calls with {\cCTOS}'s synchronous
\emph{send} and \emph{receive} calls.
We found that, when buffer size is zero, \cCTOS{} takes $3.89\mathtt{x}$ 
longer than seL4's fastpath IPC,
and $2.18\mathtt{x}$ longer than seL4's slowpath IPC. When the message size is larger than 2
words, the fastpath IPC of seL4 falls back to slowpath; in the 10-words IPC
case, \cCTOS{} takes $2.53\mathtt{x}$ longer. 
Considering \cCTOS{} requires two more system
calls than seL4, \cCTOS{}'s IPC  is slightly slower than seL4's.
Note that seL4 follows the microkernel design philosophy, and thus its IPC performance is
critical. IPC implementations in seL4 are highly optimized and heavily tailored
to specific hardware platforms. \ignore{The IPC performance of \cCTOS{} is still
practical. Verification not only should not hinder application of similar
performance optimizations, but instead provide a safety net for more aggressive
optimizations, if it is required for application scenarios of the kernel we have
in mind.}


\paragraph{Hypervisor Performance} 
\begin{figure}\centering
		\hspace{-.2cm}
		\includegraphics[width=13cm]{figs/hyp_macro.pdf}
		\hspace{-.2cm}
		\caption{Normalized macro benchmarks: Linux on KVM and \cCTOS; baseline is Linux on bare metal}
		\label{fig:eval_macro}
		\hrulefill
\end{figure}

To evaluate \cCTOS{} as a hypervisor, we measured the performance of some
macro benchmarks on Ubuntu 12.04.2 LTS running as a guest.  We ran the
benchmarks on Linux as guest in both KVM and \cCTOS{}, as well as on the bare
metal. \ignore{Although there is an unverified branch of seL4 that supports VT-x, the verified
version does not have virtualization support and cannot boot Linux. As a
result, we do not compare hypervisor performance but instead focus on a
comparison of the IPC performance of {\cCTOS} and an \emph{unverified} x86
version of seL4.}

Figure~\ref{fig:eval_macro} contains a compilation of standard macro
benchmarks: unpacking of the Linux 4.0-rc4 kernel, compilation of the
Linux 4.0-rc4 kernel, Apache HTTPerf~\cite{mosberger1998} (running on
loopback), and DaCaPo Benchmark 9.12~\cite{dacapo2006}.
We normalize the running time of the benchmarks
using the bare metal performance as the baseline (100\%). The overhead of
\cCTOS{} is moderate and comparable to KVM. In some cases, \cCTOS{}
performs better than KVM, which is mainly because KVM has a Linux host and thus has a
larger cache footprint. In other cases involving large amounts of file
operations, such as decompressing Linux source and Tomcat server, \cCTOS{}
performs worse. This is due to idiosyncrasies of the VirtIO interface. In
\cCTOS{}, we expose the raw disk interface to the guest via VirtIO; when
there is a disk I/O request in the \texttt{vring}, \cCTOS{} immediately
pulls it and issues a disk command to handle that request, polling until the
command finishes. This leads to problems with the guest Linux VirtIO driver, since
when the guest Linux uses VirtIO, it always sends disk I/O requests of length
4KB.




\input{impl}

\paragraph{Execution Model and Completeness}
The majority of the {\mCTOS} and \cCTOS{} kernels are implemented and verified 
at the C level and then compiled by a modified version of the CompCert verified
compiler~\cite{dscal15}.  The entire kernel (both C and assembly)
source code, together with the source code for the verified compiler,
are extracted into an OCaml program through Coq's extraction
mechanism. When this OCaml program is executed, the extracted C source code 
is compiled into assembly; the resulting assembly code is then merged 
with the existing assembly kernel source code to produce a single piece 
of assembly code corresponding to our verified kernel.  Thus, our deliverable 
consists of a piece of assembly code for the entire verified kernel, a 
high-level deep specification of various kernel behaviors, and a 
machine-checkable proof object stating that the assembly code running on 
the actual hardware satisfies the high-level specification.

The verified assembly code is then linked with the rest of the kernel code
(the bootloader and remaining unverified drivers) to produce the
actual binary image of the OS. The resulting kernels are practical.
\ignore{it
runs on stock x86 hardware and can successfully boot a guest version
of Linux.}

\ignore{\section{Proof effort}
We take the mCertiKOS kernel presented by Gu {\it et al}
\cite{dscal15}, and extend the kernel with various features such as
dynamic memory management, container support for controlling resource
consumption, Intel hardware virtualization support, shared memory IPC,
single-copy synchronous IPC, ticket and MCS lock implementation, new
schedulers, condition variables, FIFO blocking bounded queues, {\it
  etc}. Note that all of these new features are implemented in the
context of a concurrent machine, whereas the mCertiKOS presented by Gu
{\it et al} \cite{dscal15} only runs on a single-core machine. 

\ignore{ We
have also merged the work by Chen {\it et al} \cite{chen16} on the
interruptible kernel with device drivers using our multicore model.
}

Overall, we have contributed 3,500 additional lines of C and assembly
source code to the mCertiKOS code base. Regarding specification, there
are 943 lines of code used to specify the lowest layer axiomatizing
the hardware machine model, and 450 lines of code for the
specification of the abstract system call interfaces. These are in our
TCB and need to be trusted. We keep these specifications small to
limit the room for errors and ease the review process.  Outside the
TCB, there are 5249 lines of additional specifications for the various
kernel functions, and about 40K lines of code used to define auxiliary
definitions, lemmas, theorems, and invariants. Additionally, there are
50K lines of Coq proof scripts for proving the newly-added kernel
features. At least one third of these auxiliary definitions and proof
scripts are redundant and semi-automatically generated,\ignore{ (or
  copied and pasted),} which makes our proof a little verbose.  For
example, many invariant proofs get duplicated across the layers
whenever there is a minor change to the entire set of invariants.  We
are currently working on a new layer calculus to minimize redundant
definitions and proofs.

On the verification framework side, we developed a general linking
theorem for composing multiple threads running on the same CPU, as
well as a combining programs running on
different CPUs (10K lines).
Our team completed the verification of the new concurrent framework
and the additional features in about 2 person years. 
\ignore{It took 6 person
months to extend mCertiKOS with dynamic memory management, container
support, shared memory and synchronous IPC, and Intel
virtualization. The majority of the time (1.5 person years) was spent
on developing the framework to reason about the concurrent {\cCTOS}
kernel.} If we assume we work 250
days a year, we have written roughly 400 lines of code per
day\ignore{(50 lines per hour)}.  Thanks to the extensive layering,
the proofs for each small component are relatively easy; often it
takes less than two hours to write 400 lines of Coq code.  One of the
most challenging tasks is to come up with the correct specification
and invariants. In our experience, people make many mistakes when
initially writing down the specification; during the code verification
and refinement proof, we frequently find flaws in the specification or
notice that our invariants are too weak.  Furthermore, verification of
some components can be inherently complex, \eg, the kernel
initialization and locks.  It took us 3 days to certify the
ticket-lock acquire function (less than 10 lines of C code), and
multiple person weeks to prove that the ticket-lock is
starvation free.


Furthermore, note that we did not reach the current working solution
in one shot. We first spent about 3 person months developing an
unsuccessful version of the framework for composing multi-threaded
execution on a single CPU.  In that version, thread-local execution
was modeled using a \emph{time stamp} index into a global system
log. We eventually realized that the exact time stamps were too
cumbersome and revealed too much information about the underlying
implementation (\eg, the number of software yields within a function
body), so we spent another month developing a new system that uses
local logs (lists of events) instead.\ignore{ of time stamps, and the
  ability to shuffle and merge the events in the local logs to hide
  unnecessary nondeterminism or implementation details.}  Our initial
multicore machine model also did not work out very well when we
developed the multicore linking framework; we spent 3 person weeks to
improve the initial design through multiple iterations.  The main
challenge was finding the right invariants for the environment
context, such that we could successfully establish starvation-freedom.
}

\ignore{
\paragraph{Abstraction Layers}
\newman{I am gonna write some summary about the layered approach after I read
the overview section to see how much of the concepts are already covered.}
\ronghui{Maybe it is not helpful to include this part in the submission}
\begin{itemize}
\item Abstraction of data representation: doubly linked list --> logical list
\item Stronger invariants
\item Abstraction of primitive specification: hiding log implementation by linking and merging events
\item Refinement of machine models: from a realistic machine model to an ideal machine model that is suitable for our reasoning purpose, e.g., change in machine/memory/interrupt model
\end{itemize}
}

\ignore{
The verification effort roughly falls into three categories: layer
design with specification and invariants, refinement proofs between
the layers, and verification of C and assembly code with respect to
the specifications. The time needed for each of the categories depends
largely on the layer.  For instance, at the boundary of physical and
virtual memory management ({\code{MPTIntro}}), almost all effort
is in the refinement proof, due to the proof for the refinement between
two completely different memory models. More effort went into the
refinement proof when we introduced the Intel \emph{virtual machine
memory model}, where we proved the refinement between the concrete
four level extended page table structure in memory and the abstract
mapping from the guest addresses to the host addresses.
In contrast, for the layer {\code{MATOp}},
which initializes physical memory allocation,
most of the time was spent on verifying
the non-trivial nested loops present in the C code,
while the refinement proofs were derived automatically. 

The proofs were facilitated by automation tools for C
code, layer design patterns, and tactics libraries developed in
recent years \cite{dscal15}. These tools have greatly
reduced the amount of work needed to verify extensions of the kernel.
}


\ignore{\section{Extension and adaptation}
We augmented \mCTOSbase{} to support the hardware-assisted
virtualization technology Intel VT-x, and built a certified hypervisor
\mCTOShyper{}.  We have also built certified kernels with ring-0
process supports.  More details will be provided in the full paper.


First, we augmented \mCTOSbase{} to support the hardware-assisted
virtualization technology Intel VT-x, and built a
certified hypervisor \mCTOShyper{}. Second, we extended \mCTOSbase{}
into \mCTOSringz{} by adding support for
running certifiably-safe programs inside an ``in-kernel
process'' that runs in the privileged ring 0 mode.
We have also built \mCTOSembed{} kernel that is suitable for embedded systems,
by removing the virtual machine management and the virtual
memory management.
Removing plug-ins or layers are achieved simply by 
altering the contextual refinement proof 
at the boundary so that we can glue them back together.
}



%Bugs were found in the original design of $mCertiKOS$ during
%the C verification, which were not revealed during the original testing
%phase. They are mostly related to the overflow or the carelessness
%in implementation (e.g., writing \verb+x & y == 1+ instead of \verb+(x & y) == 1+).
% (Maybe talk about the necessity to have a separate initialization
%layer above intro layers before we introduce the layer implementing
%the actual operations???)
%Despite the frequent changes to the design, we found that the cost of change
%in our layered approach is quite small. (To compare with seL4, maybe mention
%adding new kernel modules, especially adding new kernel data structures to our
%layered design does not require significant changes to the original implementation
%of layers and the proofs.)


   % (a) Did the kernel actually run? If yes, what is the performance like?
   %     If the performance is not great, can we build a new version of
   %     our mCertiKOS kernel so that dramatically improves the performance?
   %     (note this optimized version of mCertiKOS does not have to be
   %      certified; we can explain what needs to be done in order to get
   %      this version certified and leave the actual verification to future
   %      work).

   % (b) Explain how it can manage to boot Linux; what are the key components
   %     and features in our current version of mCertiKOS that made this
   %     possible.

   % (c) Bootloader not verified, but see work by Cai et al;
   %     Stack-usage not verified, but see work by Carbonneaux et al;
   %     Interrupt handlers are ongoing work, see work by Feng et al and
   %     Guo et al. Device derivers not verified.



\ignore{
%\sectskip
\subsection{Extension and Adaptation}
\label{ssec:adapt}
%\asectskip

One primary advantage of our extensible architecture is that it makes
certified kernel extension and reasoning much easier and more principled. 
In this section, we first describe three alternative \mCTOSbase{} kernels
that we created through relatively minor changes to the base kernel. We
then present a specific example of global reasoning over the \mCTOSbase{} 
kernel~--- a simple notion of address space isolation that will serve as 
a starting point for a full-fledged security proof in the future.

We augmented \mCTOSbase{} to support the two hardware-assisted
virtualization technologies Intel VT-x and AMD SVM, and built a
certified hypervisor \mCTOShyper{}.

Fig. \ref{fig:base:vm:layers} shows the 7 layers of the virtual
machine management of \mCTOShyper{} on the Intel platform.
\code{VMInfo} is the layer object
that axiomatizes some of the hardware specific features needed
for the virtualization support. 
Since it is orthogonal to memory and process management,
the \code{VMInfo} object can be horizontally composed with the layers 
below \code{PProc} in \mCTOSbase{}.
On top of this extended \code{PProc} layer,
the virtual machine management extends the \emph{abstract memory model}
with the notions of Extended Page Table (EPT), the virtual machine
control structure (VMCS), and the virtual machine extension meta data (VMX),
which are abstracted into corresponding layer objects.
These objects are again orthogonal to the trap module above and can be
horizontally composed to export related system calls
with minimal cost.
 
\begin{figure}
\includegraphics[scale=0.33]{figs/intel_layer}	
%\vspace*{-14pt}
\caption{Layers of virtual machine management}
\label{fig:base:vm:layers}
%\vspace*{-14pt}
\end{figure}

Thanks to the contextual refinement relation we have proved for
\mCTOSbase{}, one can certify user programs using our formal
specifications of system calls. This gives end-to-end proofs on
the behaviors of user programs when they run on \mCTOSbase{}.  
Furthermore, once certified, these processes can safely run in
the privileged ring 0 mode.  We extended \mCTOSbase{} into
\mCTOSringz{} by adding support for spawning ``in-kernel
processes'' that run in the privileged ring 0 mode. 
Ring 0 processes get much
better system call performance by directly calling kernel
functions and avoiding ring switch and interrupt processing. 

The \mCTOSembed{} kernel is intended for embedded settings. To develop
this kernel we started with \mCTOSringz{} and removed the virtual
machine management, the virtual memory management, and some of the
process management layers that are related to user contexts and user
process management.  Thus \mCTOSembed{} only supports ring 0 processes
which run directly inside the physical kernel address space instead of
the user-level paged virtual address space.

Removing plug-ins or layers does not take much effort.
We only need to alter the contextual refinement proof 
at the boundary so we can glue them back together.

\paragraph{Isolation in \mCTOSbase{}}
\label{security}
We have begun exploring the verification of a global security property
on top of \mCTOSbase{}. As a starting point, we proved a basic notion
of isolation between user-level processes running in different virtual
address spaces. This isolation property is composed of two theorems:
one regarding integrity (write protection), and another regarding
confidentiality (read protection, or noninterference).  The statements
of these two theorems are as follows: suppose the top layer abstract
machine takes one step, changing the machine state from $S$ to $S'$,
and let $p$ be the id of the currently-running process (which can be
found in $S$).
\begin{description}
  \item[Integrity:]
If the value at some non-kernel memory location $l$ differs between
$S$ and $S'$, then $l$ belongs to a page that is mapped in the 
virtual address space of $p$.
\item[Confidentiality:]
\label{confidential}
If the step taken
is not a primitive call to an IPC syscall (send, recv, etc.), then the values
of memory in any address space other than $p$'s cannot have an effect on the
result of the step. In other words, if we altered $S$ 
by changing data in a different process's address space, the step would still 
have the same effect on $p$'s address space.
\end{description}

In the future, we plan to provide a more detailed security policy by
describing what can happen to confidentiality when IPC is used.  This
description will be expressed in terms of propagation of security
labels on the IPC data. Note, however, that our framework allows for
security labels to be specified at a purely logical level~--- there is
no need for concrete representation and manipulation of labels at run
time.

Noninterference properties are generally not preserved across
refinement due to nondeterminism. It may therefore seem that the
aforementioned \emph{confidentiality} holds only at the topmost layer,
but not at lower layers. It turns out, however, that our notion of
deep specification is strong enough to preserve
noninterference. Essentially, to give a deep specification to a
nondeterministic semantics, we must first externalize the source of
nondeterminism (e.g., into an oracle). The noninterference property
then becomes parameterized over this source of nondeterminism, which
allows the parameterized property to be preserved across
refinement. This relationship between deep specification,
noninterference, and refinement will be explored comprehensively in
future work.
}

