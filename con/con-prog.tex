\section{Layered Concurrent Programming}
\label{sec:prog}

\ignore{
\paragraph{outline} 1) C-level vs assembly-level programming
2) layer refinement for composing multiple events?
3) spinlocks (ticket lock, MCS lock)
4)thread management
5) queuing locks
6) starvation-free condition variable
}


In this section,\ignore{we instantiate our framework with a
C-like language ClightX and an assembly level language LAsm,
and we introduce the concurrent layer interfaces for
a single active CPU.
To demonstrate the modularity and practicality of our framework
for concurrent program reasoning,}
we show how to verify spinlock implementations, shared queues protected by spinlocks,
thread scheduling primitives, and a queuing lock,
using concurrent abstraction layers.
All layers are built upon the single-core machine $\LAsm{(L(c,\oracle))}$.

\ignore{
\subsection{Layer interfaces}
\label{sec:prog:layer}
We enable abstraction in our framework by
instrumenting concrete memory states with abstract
state and logical log components. 
Both abstract state and logical logs can be manipulated using
primitives, which are made available through CompCert’s external
function mechanism.

\paragraph{Machine state}
for a single core $c$ 
 is defined as $\state:=(\regs, m, a, l)$,
which consists of the CPU-local state $\regs$,
shared memory state $m$, 
abstract state $a$, and a global log $l$. 
%The assembly language
%contains an extra register set component $regset$.

\textbf{Local state} $\regs$ consists of the CPU-local memory state,
which is invisible to other CPUs,
and the language-dependent local components.
\ignore{The local memory is a sequence of CompCert memory blocks\ronghui{reference here, Tahina} \tahina{Do we really need to say here that we are using the CompCert memory model?},
which can be addressed by the block identifier and the offset within the block. This part of the memory is  private to a single CPU,
and are invisible to other CPUs.
The rest of the local components are language dependent.}
For instance, a C-like language includes 
a local/temporary environment and a continuation,
while an assembly language contains a register set.

\textbf{Shared memory state}  $m$ is \ignore{also  a sequence of
memory blocks, which are }shared among all CPUs.
Each shared memory location is associated
with a valid bit, which can only be manipulated by
atomic shared operations (\ie, $\pull$ and $\push$). The $\pull$ operation
sets an invalid bit to valid, only after which can shared memory accesses
be performed. The $\push$ operation invalidates the bit
and records the shared memory updates to the logical log.
If a program tries to pull a shared memory location that is already valid,
there is a \emph{data race} and the machine gets \emph{stuck}.
One goal of concurrent program verification is to show
that a program is data-race free.
In our setting, we accomplish this by showing that the program does not 
get stuck.

\textbf{Abstract state} $a$ is not just a ghost state for reasoning.
Instead, it is abstracted from a concrete in-memory data structure,
and it can influence the outcome of program executions.
It is used to hide implementation details and to lift reasoning
to a purely logical level. Access to an abstract state can only
be performed via explicit calls to primitives.
\ignore{
\newman{Duplicated}
, which are implemented
as Clight’s external function calls. This minimizes the impact on the
semantics of C and assembly languages.
}

\textbf{Global log} 
is a list of events. It records all shared memory operations that affect more than
one CPU in the machine. Events generated by different CPUs are
interleaved in the log, following the actual chronological order of events.
The logical log is also used as an index to query the environment context
$\oracle$, which returns the next event \newman{always one???} generated by 
environment CPUs.


\paragraph{Primitives and layer interfaces}
Primitives are the only way to access the abstract state and
logical log. CompCert offers a notion of external
functions, which are useful in modeling interaction with the environment.
Similarly to Gu et al.'s ClightX and LAsm \cite[\S 4, \S 5]{dscal15}, we use the external function mechanism to extend different languages with our primitive operations. This minimizes the impact on the semantics of C and assembly languages.
There are two types of primitives:
\emph{private} primitives only access the CPU-local state
and the shared memory locations that are $\comm{valid}$;
\emph{shared} primitives can access the global log
and generate events.

\ignore{A primitive is called an atomic shared operation
if all the events generated by the primitive are always adjacent in the logical log.
In other words, its execution can be viewed as atomic
on the concurrent machine.}

The type of abstract state $A$, potential events of the global log
$E$,
and the set of available primitives $P$ constitute our notion of layer
interface $L := (A, E, P)$.
\david{We used $A$ for active set and $P$ for program previously;
consider using new letters here.}

\paragraph{Semantics}
We present the semantics of programs running on a single CPU with ID $c$ in
a big-step manner.
In the rest of this section,
we denote $\oracle$ as an abbreviation
of $\oracle_c$, which is the environment context
for CPU $c$.
We write $ L, M, \oracle, c \vdash f : (args;\state) \Downarrow (res;\state')$ to denote
that under the environment context $\oracle$, a function $f$ defined as either
an internal function in the module $M$ or as a primitive in the layer interface $L$,
when called with argument list $args$ from the machine state $\state$, transforms
the machine state to $\state'$ with return value ${res}$.

\textbf{Internal function} semantics are defined in terms
of execution of the function body.
We write $L, M \vdash S : (\regs, m, a) \downarrow 
(res; \regs', m', a')$
for the semantics of statements: from the local state $\regs$,
the shared memory state $m$, and the abstract
state $a$, execution of $S$ terminates with return value
$res$, and yields the local state $\regs'$,
the memory state $m'$, and the abstract
state $a'$. Note that only the $\comm{valid}$ parts of the shared memory 
state can be accessed by statements.
\begin{small}
\[
\inferrule{
  M(f) = S \\
  \regs_0 = \regs \leftarrow args \\ 
  L, M \vdash S : (\regs_0, m, a) \downarrow 
(res;\regs', m', a')
}{
  L, M, \oracle, c \vdash f : (args;\regs, m, a, l) \Downarrow (res;\regs', m', a', l)
}
\]
\end{small}

\textbf{Primitive calls} simply query the layer interface $L$
with a particular environment context $\oracle$.
\begin{small}
\[
\inferrule{
 \oracle, c \vdash  L(f)(args;\state, res;\state')
}{
   L, M, \oracle, c \vdash f : (args;\state) \Downarrow (res;\state')
}\]
\end{small}

\ronghui{This part needs revisit after Sec.3 is finished}
In our single-core machine model,
all the potential interleaving
is \emph{delayed} until the invocation of
shared primitives (\cf \ronghui{Sec.3's set up}).
Thus, statements and private primitive calls
can only access part of the state (\ie, $(\regs, m, a)$)
and similar to the sequential case.
The shared primitive
has to deal with all the delayed interleaving
(denoted as ``$\intp$" before it)
by querying the environment context $\oracle$
with the current global log $l$
and update $l$ with the returned events generated by the environment.
In most cases, the execution of a primitive depends on what events have
been triggered at that time. 
For example, the $\pull$ primitive returns the
current state of the shared memory, which depends on what shared
memory updates from other CPUs are recorded in the global log.
To write the primitive specifications, we introduce an idiom of a
\textbf{replay function} $\replay$, which takes a
log as an argument, and interprets it to calculate the ``current
state'' of the system after those events have happened. The
result of the replay function can be an arbitrary type, corresponding
to what kind of information is needed about the shared operations.

For example, for the shared memory operations, function
$\replay_{\comm{get\_shared}}$  is defined to retrieve the last shared memory
update to location $b$ from the log $l$. If the last update is
$c.\pull(b)$, it returns $(\any, \comm{invalid})$; if the last update
is $c.\push(b,\comm{bl})$
for some byte list $\bytelist$, 
 it returns $(\comm{bl}, \comm{valid})$;
otherwise, it returns $(\any, \comm{valid})$.
We write ``$f\set{b:v}$"
to denote an update to the partial function $f$ at field $b$ 
with value $v$.
Thus, the rules for $\pull$ and $\push$
are defined as below:
\begin{small}
\begin{mathpar}
\inferrule{
   l' = l \cons \oracle(l) \cons c.\pull(b) \\ 
  (\bytelist,  \comm{invalid}) = \replay_{\comm{get\_shared}}(l', b) \\
  	m' = m \set{b: \bytelist} \\
  	a' = a \set{\perm{b}: \comm{valid}}  	
}{
 \oracle, c \vdash  \spec_{\pull}([b],\regs, m, a, l,\any, \regs,  m', a', l')
} 
\and
\inferrule{
  m(b) = \bytelist \\
  a.\perm{b} =  \comm{valid} \\
  a' = a \set{\perm{b}: \comm{invalid}} \\
  l' = l \cons  \oracle(l) \cons c.\push(b,\bytelist)   
}{
 \oracle, c \vdash  \spec_{\push}([b], \regs, m, a, l,\any, \regs, m, a', l')
}
\end{mathpar}
\end{small}

The $\pull$ operation first queries the environment context to update
the logical log, and updates the shared memory at 
location $b$ with the byte
list $\bytelist$ retrieved from the log. This specification
captures the fact that the shared memory contents may be
modified by the environment context before the execution of
primitives.

\subsection{Concurrent Layered programming and verification}
\label{sec:prog:example}}

\ignore{In this section, we show how to build concurrent abstraction
layers for verifying concurrent objects.}

\paragraph{Ticket lock}
\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/ticket_lock.c}
\vspace{-10pt}
\caption{Pseudocode of ticket lock implementation.}
\label{fig:exp:ticket_lock}
\vspace{-10pt}
\end{figure}

is a standard mutex algorithm, which ensures that CPUs acquire the
lock in the same order as they started waiting for it (fairly).
Figure~\ref{fig:exp:ticket_lock} shows the implementation in
pseudocode. We provide a set of locks indexed by integer $b$
(the memory location the lock protects). 
\ignore{The state of each lock $b$ is stored in two integers, 
$\now \le \ticket$.} To acquire a lock, a CPU uses the atomic
fetch-and-increment instruction to obtain a ticket number, and then waits
for all previously-waiting CPUs to release the lock in turn, until
{\tt now} equals the ticket number.
Per Gu et al.'s methodology \cite{dscal15}, we begin by 
introducing the underlay interface,
then we write a
low-level functional specification for the lock implementation,
and finally we abstract this specification into a short
high-level description.
\vspace{3pt}

\noindent\textbf{Step 1 (memory operation interface $L_0$).}
Because the $\comm{ticket\_lock}$ struct is used for communication
between concurrent code, 
the bottom layer interface $L_0$
has to define the globally-visible events that can be generated. 
When writing the specification of $L_0$,  we can choose the type of events in any way, as long
as it is 
fine-grained enough that we can state all scheduling interleavings that
the code may encounter. In this case, we have events for the updates to
$\ticket$ and $\now$ and an event for reading the $\now$ value
for a lock $b$. Additionally, we make use of a natural number 
$n$\ignore{ and an event $\holdlock$}, which has no
operational significance and is explained below.
\begin{small} 
\[
\begin{array}{c}
L_0.\Ev := \incticket(n, b) \mid \incnow(b) \mid \getnow(b)
\mid \push(b,bl) \mid \pull(b)
\ignore{\\
&  \mid &  \holdlock \mid \tshared (e : \mathsf{SharedMemEvent})}
\end{array}
\] 
\vspace{-10pt}
\end{small}%

To specify the meanings of the get and increment events, we define a replay
function $\replay_{\comm{ticket}} : Log \to \integer \to \integer\times \integer$. 
The definition is as one would expect:
$\replay_{\comm{ticket}}(\mathsf{nil}, b) = (0,0)$, each 
$\incticket(n, b)$ and $\incnow(b)$ event in the list increments
the first or second component of the pair, and other events are ignored.

We then specify the behavior of the primitives
of $L_0$, which are implemented by the atomic instructions on x86 architecture. 
For example, we implement the
fetch-and-increment $\mathsf{FAI\_ticket}$ primitive in a small 
assembly snippet, yielding the following specification:
%% Is this just assumed, or is it proved somewhere?
\begin{small}
\begin{mathpar}
\inferrule{
 l_0 = l \cons \oracle(l) \\
  \replay_{\comm{ticket}}(l_0, b) = (i,j) \\
  l' = l_0 \cons c.\incticket(n, b)\\ 
}{
 \oracle, c \vdash  \spec_{\mathsf{FAI\_ticket}}([n, b], \regs, m, a, l, i, \regs, m, a, l')
}
\end{mathpar}
\vspace{-5pt}
\end{small}%

In other words, this primitive queries $\oracle$, replays the result log to
compute the current state $i$ of {\tt ticket},
and then appends one
 event to the log. 
\ignore{The fact that we are
using an atomic memory instruction, and that such memory updates are
immediately visible to all CPUs, is reflected by the
primitive just adding a single event so there can be no unexpected
interleavings between CPUs. We add similar specifications for the
other operations, \eg, a $\sigma_{\mathsf{get\_now}}$ primitive whose
implementation is a memory read, and whose specification queries
$\oracle$ to get the new log $l_0$, appends $\getnow (b)$, and returns
$\mathsf{snd}(\replay_\mathsf{ticket}(l_0, b))$.}
The primitive set of $L_0$ is defined as below:
\begin{small}
\[
\begin{array}{rl}
L_0.\primt := &\mathsf{FAI\_ticket} \mapsto \spec_{\mathsf{FAI\_ticket}}
\oplus \mathsf{inc\_now} \mapsto \spec_{\mathsf{inc\_now}} \\
&\oplus \mathsf{get\_now} \mapsto \spec_{\mathsf{get\_now}}\oplus  \push\mapsto \spec_{\push}
\oplus  \pull\mapsto \spec_{\pull}
\end{array}
\]
\end{small}%
\noindent\textbf{Step 2 (low-level specification $L_1$)}
In the next layer $L_1$, 
we verify the implementation of the ticket lock based on
 $L_0$. Since each lock is associated with a shared variable,
after the lock is acquired, it invokes $\pull(b)$ 
to update the shared memory at the corresponding location $b$
according to the log.
The bodies of the lock acquire function $\acq$ and 
the lock release function $\rel$ are verified as if they were 
sequential C programs. Uses of the lock primitives are handled
similarly to external function calls, by applying their
specifications. From the verifier's point-of-view, they are no 
different from code
that writes to a thread-local variable (namely, the log). 

However, we need some care to assign a good specification to these
functions. While $\rel$ is straightforward (just append the
$\push$ and $\incnow$ events to the log), 
$\acq$ contains a while-loop. To prove total correctness,
we would like to assign a specification which later can be proven
to terminate.

We define an auxiliary function $\comm{wait}$
(using recursion in Coq) describing the behavior of the first $k$
iterations of the loop: each time we query the environment
context $\oracle$, it generates its own
event, and checks if CPU number $c$ is due to hold the
lock. The function is undefined (Coq {\tt None}) if we do not get the
lock within $k$ iterations.
\begin{small}
\[
\begin{array}{lcl}
\mathsf{wait}(0, \oracle, c, b, l) & = &\text{undefined} \\
\mathsf{wait}(k+1, \oracle, c, b, l) & = & l', \text{ if $l'$ indicates next is $c$'s turn to get lock}\\
                       & &   \mathsf{wait}(k, \oracle, c,  (l' \cons c.\getnow(b))), \text{ otherwise} \\
                       & &  \text{where }l' = l \cons  \oracle(l)
\end{array}
\]
\end{small}%

We then define the semantics of the primitive by saying that it first
generates an $\incticket$ event, and then loops
for some ``sufficiently'' large number $\calwait(l, c)$ of iterations.
\begin{small}
\begin{mathpar}
\inferrule{
  l_0 = \mathsf{wait}(\calwait(l, c), \oracle, c, b,
  	l \cons \oracle(l) \cons c.\incticket(n,b)) \\
  (\bytelist,  \comm{invalid}) = \replay_\comm{get\_shared} (l_0, b) \\
  	m' = m \set{b: \bytelist}\\
  	a' = a \set{\perm{b}: \comm{valid}} \\
  	l'  = l_0 \cons c.\pull(b) \\
}{
 \oracle, c \vdash  \spec_{\acq\_\mathsf{low}}([n,b], \regs, m, a, l,\any, \regs, m', a', l')
}
\end{mathpar}
\end{small}

Computations where $k$ reaches 0 are considered crashing, and our
ultimate theorem is about safe programs, so when proving that the C
code matches the specification we only need to consider finite
successful runs. It is easy to show that the C loop can match any of
those, since it can run any number of times.

To prove the \textbf{starvation-freedom} of ticket lock,
we have to know what number $\calwait(l,c)$ is sufficiently large.
Thus,  we need more information than just the two integers representing the lock
in memory. In this layer $L_1$, we introduce
a new  replay function:
{\small
\[
\replay_{\mathsf{wait\_q}} : Log \to  \mathsf{LockStatus} \times
 \mathsf{list}\ (\mathsf{CPU\_ID}\times\mathbb{N})
\]}
This computes which CPU (if any) currently holds the log, and  a list of all CPUs that have started waiting for
the lock together with the natural number $n$ that
$\spec_{\acq}$ was given as an argument. The meaning of
$n$ is that once the CPU enters the critical section, it promises to take  at most $n$ steps before exiting. 
The replay function $\replay_{\mathsf{wait\_q}}$ is defined by going through the log
event by event, for example $c.\incticket(n,b)$ appends $(c,n)$ to the list
of waiting CPUs.
Finally, in the proof we assume that the hardware scheduler is \emph{fair}, in the sense that there exists some 
constant $n_0$ such that any CPU will be rescheduled within $n_0$ steps. 
Then we can calculate maximum wait-time $\calwait(l, c)$ as $n_0$ times the sum of all the bounds $n$ for the CPUs waiting ahead of $c$ in the queue.

%% held'' to ``held'' and sets the ``time to finish'' $n_f$ to $n$. 
%% Assume the hardware scheduler is \emph{fair},
%% such that any CPU will be rescheduled within $n_0$ steps.
%% Thus, each event
%% generated by the CPU that holds the lock decrements the ``time to
%% finish'' counter $n_f$ (and resets the ``time until scheduled'' counter $n_s$ to be $n_0$),
%% while each event generated by some \emph{other} CPU decrements the
%% ``time until scheduled'' $n_s$. 
%% Let $\replay_{\comm{wait\_q}} (l) := (n_f, n_s, \any, q)$
%% for the current global log $l$.
%% By the fact that
%% $(n_f \le q[0].n)$ and 
%% $(n_s \le n_0)$, the \emph{maximum} wait-time $\calwait(l, c)$ can be calculated as below:
%% {\small
%% \[
%% \calwait(l, c) = n_0 + n_0 \times \sum_{n_i}
%% (i, n_i) \text{ for all CPU waiting before }c\text{ in }q
%% \]}

Let $L_1.\primt := \acq \mapsto \spec_{\acq\_\mathsf{low}}
\oplus \rel \mapsto \spec_{\rel\_\mathsf{low}}$
 and  $M_0 := \acq \mapsto \code_{\acq}
\oplus \rel \mapsto \code_{\rel}$,
we can build the concurrent layers as below:
\begin{small}
\[
\forall \oracle, 
\ltyp{(L_0, \oracle)}{\id}{M_0}{(L_1, \oracle)}
\]
\end{small}%
Note that,
because the abstract state and the global log are identical
in both layers, the simulation relation is the identity relation $\id$.

\vspace{3pt}
\noindent\textbf{Step 3 (atomic specification $L_2$)}
All the above details are irrelevant to clients using the lock, who only
need to know that they are allowed to get the lock as long as they
promise to release it within $n$ steps. We introduce a new layer $L_2$ to
suppress the details, where we use a simpler type of events and replay function.
{\small
\[
\begin{array}{l}
L_2.\Ev := \acq(n,b) \mid \rel(b,bl)
\qquad
L_1.\primt := \acq \mapsto \spec_{\acq}
\oplus \rel \mapsto \spec_{\rel}\\
\replay_\mathsf{lock} : Log \to \mathbb{N} \times
\mathsf{LockStatus}
\end{array}
\]}%
The replay function computes which CPU (if any) currently holds the
lock, and has a counter for how many more steps that CPU is allowed
to take. Again, this is computed event by event: if CPU $c$
generates the event $c.\acq(n,b)$, the high-level interpretation of the
log becomes $(n, \mathsf{held\_by}\ c)$ for lock $b$, and then 
 the counter decrements each time CPU $c$ takes a step. 
 If the counter
ever reaches $0$ the program is considered \emph{crashed}, so clients of the
lock need to prove that they respect the bound $n$ they
provided. (Typically this is easy, because most clients only carry out
some constant number of operations in the critical section.)

The specifications for the high-level lock and unlock primitives are
now very simple:
\ignore{We introduce a new abstract state $\lockstatus$,
which is a set of boolean saying
whether this CPU holds the lock (to express the precondition that you
can only get/release the lock if you are outside/inside the critical
section).  Thus, }$\acq$ and $\rel$ just query the environment context, and then
introduce one more event.
\begin{small}
\begin{mathpar}
\inferrule{
	l_0 = l \cons \oracle(l)\\
  (\bytelist,  \comm{invalid}) = \replay_\comm{get\_shared}(l_0, b) \\
  	m' = m \set{b: \bytelist}\\
  	a' = a \set{\perm{b}: \comm{valid}} \\
  	l' = l_0 \cons c.\acq(n,b) \\
}{
 \oracle, c \vdash  \spec_{\acq}([n,b], \regs, m, a, l,\any, \regs, m', a', l')
}
\\
\inferrule{
  m(b) = \bytelist \\
  a.\comm{perm}(b) = \comm{valid} \\
  a' = a \set{\perm{b}: \comm{invalid}} \\
  l' = l \cons \oracle(l) \cons c.\rel (b, \bytelist)
}{
 \oracle, c \vdash  \spec_{\rel}([b], \regs, m, a, l,\any, \regs, m, a', l')
}
\end{mathpar}
\vspace{-5pt}
\end{small}

By having these primitives introduce only a single event, clients of the
lock code can treat the lock operations as completely \emph{atomic}; there is
no need to consider interleavings of the ``get lock'' execution
with events from other CPUs. In order to prove that this refinement
is sound, we must provide a function $f_{l1}$ from logs (and environment
contexts) at the lower layer to logs at the higher layer.  In this
case, $f_{l1}$ goes through the log and discards all events except for
$\pull$ and $\incnow$, which get mapped to $\acq$ and
$\rel$.
The function $f_{\oracle1}$ to convert the $\oracle$ 
and the simulation relation $R_1$ are both defined using $f_{l1}$.

The simulation proof
includes showing that $\spec_{\acq\_\mathsf{low}}$ is defined whenever
$\spec_{\acq}$ is, so we need to prove that the function
$\mathsf{wait}(\calwait(l, c), \oracle, c, b, l)$ returns a value. This is done
by computing a natural-valued termination metric for the log 
(in terms of
$\replay$), and showing that this decreases faster than $\calwait(l,c)$. In other words,
because our high-level specification of the lock is a total function,
proving that the implementation is correct includes a proof of
liveness. Thus,
{\small 
$
\forall \oracle, 
\ltyp{L_1(c, \oracle)}{R_1}{\emptyset}{L_2(c,f_{\oracle1}(\oracle))}
$}.%

This simulation proof is only between specifications.
By applying the vertical composition rule \ignore{ (Lemma~\ref{lem:vertical})}, we have:
\begin{small}
\[
\forall \oracle, 
\ltyp{L_0(c,\oracle)}{R_1}{M_0}{L_2(c, f_{\oracle1}(\oracle))}
\]
\vspace{-10 pt}
\end{small}

\ignore{
\paragraph{MCS lock}

Ticket locks are inefficient in systems with very many processors (10
to 40, depending on workload~\cite{Boyd-wickizer12}), because every
CPU repeatedly reads the same memory location. As an alternative, we
implement the MCS algorithm~\cite{mcs91}. In this algorithm,
different CPUs competing for the lock each own one node of a
linked list in memory, which represents the queue of waiting CPUs. The
lock also contains a pointer to the tail of the list, which is shared
between all CPUs and has to be manipulated using atomic
compare-and-swap instructions. To take the lock, a CPU only
swaps the tail-pointer once, then modifies the next-pointer
of the previous node, and waits for the previous CPU to modify its own
node. The MCS lock is verified in our framework following similar
strategies as ones used for the ticket lock. However, we have to deal
with extra level of complexity as the swap and modify events can be arbitrarily
interleaved. For example, the next-pointers in the list are not always
valid, and may be updated in any order.
}

% Sigh, I can't get this to look nice. 
\ignore{
\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/mcs_lock.c}
%\vspace{-5pt}
\caption{Pseudocode of MCS-lock implementation.}
\label{fig:exp:mcs_lock}
%\vspace{-10pt}
\end{figure}
}

\ignore{
Compared to ticket locks, the MCS algorithm is more involved, since
the swap and modify events can be arbitrarily interleaved. For
example, the next-pointers in the list are not always valid, and may
be updated in any order.  This makes it a good example, showing that
our verification methodology works for interesting concurrent
programs. Previously, this algorithm had only been proven correct
using pen-and-paper.

The verification strategy is very similar to the ticket locks,
and the structure provided by abstraction layers is even more helpful here. In the
memory operation layer, instead of calculating a pair of integers, the
replay function calculates a value of type 
$\integer \times (\mathsf{Map}\ \integer\ (\mathsf{bool}\times \integer))$, that is
the value of the tail pointer, and a finite map representing the list
nodes.  The rest of the proof does not need to concern itself
with a low-level memory model, and reasoning about the
replay function can be done similarly to reasoning about a pure
functional program.

The interface provided by the mid-level layer is then very similar to
the mid-layer $L_1$ for ticket locks. Again, the state of the lock
can be summarized as a logical list of waiting CPUs and counters
bounding the number of operations they may perform. In order to capture the
additional complexity of next-pointers, we only need one more
component (a set of nodes). The proof of starvation freedom is quite
similar to the ticket lock, and the high-level interface $L_2$ is
identical to that exposed by the ticket locks.
}

\paragraph{Shared queue object}
is widely used in concurrent programs, e.g., as buffers for producer/consumer,
as the list of threads in a scheduler, {\it etc}.
\ignore{The specification of a shared queue object should provide a high level abstract
interface by hiding all the low level details,
to ease the verification of programs using the object.
However, the large gap between this high level abstract specification
and the low level efficient implementation makes the verification of the shared queue object
itself extremely challenging.
Furthermore,}
In previous work~\cite{lili16},
due to the lack of layering support,
the verification of any shared object
has to inline the lock implementation
and duplicate the lock-related proofs.
In this example, we illustrate how we utilize concurrent
abstraction layers to simplify the verification of a shared queue object 
implemented with fine-grained \emph{spinlocks}, which is provided by the layer $L_2$.
	
\vspace{3pt}
\noindent\textbf{Step 1 (intermediate layer $L_3$)} 
Figure~\ref{fig:exp:dequeue} shows
the implementation of a dequeue operation $\deq$ in terms of a doubly linked
list.
It first acquires the spinlock associated with queue $i$
(via the auxiliary function $\qloc$),
then invokes the local dequeue operation $\deq\comm{\_t}$
and releases the lock.
\ignore{Suppose the underlying layer interface is:
$$L_1.P = \acq \mapsto \spec_{\acq}
	\oplus \rel \mapsto \spec_{\rel}$$}
Instead of directly verifying the function $\code_{\deq}$
with fine-grained locking in one shot, we do it over
multiple layers.
We first introduce an intermediate function
$\code_{\deq\comm{\_t}}$ (\cf Fig.~\ref{fig:exp:dequeue}), which contains
the code that performs the local dequeue operation but assumes that
the corresponding lock is held.
Verification of the function $\code_{\deq\comm{\_t}}$ is now much
simpler because it can be treated as a sequential program.
For this purpose, we introduce a new layer interface $L_3$ as follows:
(1) introduce abstract states $abs_\comm{tcbp}$ and $abs_\comm{tdqp}$;
(2) define a simulation relation $R_{2}$ between the new abstract states
and the concrete doubly-linked list in memory;
and (3) prove the simulation between
$\code_{\deq\comm{\_t}}$ and its specification
$\spec_{\deq\comm{\_t}}$.

The newly introduced $(abs_\comm{tdqp}: \integer \partf 
\comm{List\ \integer})$ is a partial map from \emph{queue index}
to the \emph{abstract queue}, which is represented as
a list of indices to thread control blocks (\ie, $\comm{tcb}$).
The specification $\spec_{\deq\comm{\_t}}$ is defined as below:
\begin{small}
\begin{mathpar}
\inferrule{
 \replay_\comm{lock\_status} (l,\qloc(i)) 
 = \comm{held\_by}\ c \\
 (a', res) =
 (\comm{if}\ a.\comm{tdqp} (i) = r\cons q 
\ \comm{ then }\  (a.\comm{tdqp}\set{i: q}, r)
 \ \comm{ else}\ (a, -1))
}{
 \oracle, c \vdash  \spec_{\deq\comm{\_t}}([i],\regs, m, a, l,res, \regs,  m, a', l)
}
\end{mathpar}
\end{small}%
The  simulation relation 
$R_{2} (s_2, s_3)$ between $L_2$ and $L_3$
%$R_{12}: L_1.\comm{state}\rightarrow L_2.\comm{state} \rightarrow \comm{Prop}$
is:
\vspace{-5pt}
\begin{itemize}
\itemsep0em
\item Memory and abstract states are the same, except for $\comm{tcbp}$,  $\comm{tdqp}$.
\ignore{,
$abs_\comm{tcbp}$,  and $abs_\comm{tdqp}$}
\item There is no permission for $\comm{tcbp}$
and $\comm{tdqp}$ in $s_3.m$.
\item There exist functions $f_\comm{tcbp}$
and $f_\comm{tdqp}$, which map the thread control blocks and the doubly linked list in $s_2.m$ to $s_3.a$.
\item There exists a function  $f_{l2}(s_2.l) = s_3.l$,
which maps the events $c.\acq(n,\qloc(i))$ and $c.\rel(\qloc(i), bl)$
to $c.\acq_q(n,i)$ and $c.\rel_q (i, f_\comm{tdqp}(bl))$,
respectively.
\item  As for environment context,  $f_{\oracle 2}$  can be defined using $f_{l2}$.
\end{itemize}
\ignore{
as:
\begin{small}
\[
f_{\oracle 2} (\oracle) (l, c, b)=
\begin{cases}
f_{l2} (\oracle(l', c, b)) & \text{if } \exists l', f_{l2}(l') = l \wedge \reach(\oracle, c, l')\\
[] & \text{otherwise}
\end{cases}
\]
\end{small}

Note that since $\oracle$ represents the strategy of the environment
context, not all the logs are \emph{reachable} starting from the empty log.
This reachable set is defined as:
\begin{small}
\begin{mathpar}
\inferrule{ }
{
\reach(\oracle, c, [])
}
\and
\inferrule{\reach(\oracle, c, l) \\ b = \comm{block} (e)}
{
\reach(\oracle, c, c.e \cons \oracle(l,c,b) \cons l)
}
\end{mathpar}
\end{small}
If both $l$ and $l'$ are reachable logs from $\oracle$,
we can prove that $f_{l2} (\oracle(l, c, b)) =f_{l2} (\oracle(l', c, b))$. Therefore,  $f_{\oracle2}$ is well-defined.}

Let $L_3.\primt := L_2.\primt \oplus \deq\comm{\_t} \mapsto \spec_{\deq\comm{\_t}}$ 
and
$M_2 :=  \deq\comm{\_t} \mapsto \code_{\deq\comm{\_t}}$.
We build the concurrent abstraction layer:
\begin{small}
\[
\forall \oracle, 
\ltyp{L_2(c,\oracle)}{R_2}{M_2}{L_3(c,f_{\oracle 2} (\oracle))}
\]
\vspace{-10 pt}
\end{small}%
\ignore{This simulation is proved
by showing that $s_2.l$ is reachable from $\oracle$
and that $f_{l2}(s_2.l) = s_3.l$ always holds.
\david{I don't follow this sentence, it seems unnecessary.}}

\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/dequeue.c}
\vspace{-5pt}
\caption{Pseudocode of dequeue implementation.}
\label{fig:exp:dequeue}
\vspace{-10pt}
\end{figure}

\noindent\textbf{Step 2 (atomic specification $L_4$)}
Next we verify the actual function $\code_{\deq}$ that wraps
$\code_{\deq\comm{\_t}}$ with the lock primitives.
We build a new layer interface $L_4$ that merges the two lock events
$\acq$ and $\rel$ into a single event $\deq$,
by showing that the dequeue operation, protected with spinlocks,
satisfies the atomic specification $\spec_{\deq}$ defined as:
\begin{small}
\begin{mathpar}
\inferrule{
 l_0 = l \cons \oracle(l) \\ 
 \replay_\comm{get\_queue} (l_0, i) 
 = q \\
res =
 \comm{if}\ (q = r\cons q') 
\ \comm{ then }\  r
 \ \comm{ else}\ -1 \\
  l' = l_0 \cons c.\deq(i)
}{
 \oracle, c \vdash  \spec_{\deq}([i],\regs, m, a, l,res, \regs,  m, a, l')
},
\end{mathpar}
\end{small}%
where $\replay_\comm{get\_queue}$
calculates the queue of CPUs waiting for the lock.

The simulation relation 
$R_{3}$ between $L_3$ and $L_4$ is trivial,
except the log function $f_{l3}$.
It drops the event $c.\acq_q(n,i)$
and maps the event $c.\rel_q(i, q)$
to $c.\deq(i)$. Thus, the function $f_{\oracle 3}$ to convert
$\oracle$ can be defined using $f_{l3}$.
\ignore{
\begin{itemize}
\itemsep0em
\item The memory and abstract state are the same, except for  $abs_\comm{tdqp}$.
\item $s_3.a.\comm{tdqp} (i) = 
L_4.\replay_\comm{get\_queue} (s_4.l, i) $,
which means that the queue constructed from
$s_4.l$ is equal to the queue in $s_3.a$.
\item There exists a function $f_{l3}(s_3.l) = s_4.l$,
which omits the event $c.\acq_q(n,i)$
and maps the event $c.\rel_q(i, q)$
to $c.\deq(i)$. 
Constructing  $f_{\oracle 3}$ from $f_{l3}$ is similar to
the ticket lock example.
\end{itemize}
\ignore{Therefore, we can define the function $f_{\oracle 1}$ for environment context:}}
Let $L_4.\primt := L_3.\primt \oplus \deq \mapsto \spec_{\deq}$ 
and
$M_3 :=  \deq \mapsto \code_{\deq}$.
We build the concurrent abstraction layer:
\begin{small}
\[
\forall \oracle, 
\ltyp{L_3(c, \oracle)}{R_3}{M_3}{L_4(c, f_{\oracle 3} (\oracle))}
\]
\end{small}%
Vertical composition \ignore{ (Lemma~\ref{lem:vertical})} then yields the desired result:
{\small
\[
\forall \oracle, 
\ltyp{L_2(c,\oracle)}{R_3\cdot R_2}{M_2 \oplus M_3}{L_4(c,f_{\oracle 3} \cdot f_{\oracle 2}(\oracle))}
\]
\vspace{-10pt}
}%

\paragraph{Scheduler primitives and multi-thread composition}
It is common for multi-threaded programs to perform explicit synchronization
of threads by calling scheduler primitives, e.g., $\yield$, $\sleep$, and $\wakeup$.
Verification of such concurrent programs is extremely challenging.
Previous work either directly models the abstract scheduler primitives at a high level
(e.g., simple switch in the thread ID~\cite{xu16}),
or verifies the scheduler functions with a low level small-step semantics
(e.g., saving and restoring the register set) \cite{dscal15}.
Both approaches have severe limitations. The first approach provides no formal connection
between the specification and underlying C and assembly implementation of those
scheduler functions. In the second approach, the specifications are too low level to
be useful in verifying the actual programs calling these primitives. Furthermore,
some portion of scheduler functionality is implemented in assembly (e.g., context switch).
Thus the semantics does not satisfy the C calling convention, meaning that it 
is impossible to reason about programs calling these assembly functions at the C level.

In this example, we build concurrent layers for the assembly part
of a scheduler in a small-step manner, but later lift the specifications
to big-step ones satisfying the C calling conventions. 
This allows us to perform \emph{thread-local} reasoning of programs
at the C level, where the proved properties of each thread can be linked formally
to obtain a global claim about the whole set of threads running on a processor.
To the best of our knowledge, this is the first attempt to support modular verification
of multi-threaded applications with explicit synchronization using verified
low level scheduler primitives.

\vspace{3pt}
\noindent\textbf{Step 1 (assembly-level specification $L_5$)} 
Figure~\ref{fig:exp:sched} shows the implementation
of selected scheduling functions.
The $\yield$ function first polls a pending
thread from the per-CPU pending queue
(\ie, $\comm{pendq}(c)$),
then selects the next running thread
from the CPU-local ready queue
(\ie, $\comm{rdq}(c)$),
and finally switches the kernel context.
The $\sleep$ function is similar: it
puts the current running thread
on the sleeping queue (\ie, $\comm{slpq}(q)$) and
then context switches.
The $\wakeup$ function moves the head of the sleeping queue
into the corresponding CPU's pending queue.
The context switch function $\mathsf{cswitch}$
saves the current thread's kernel context (including the 
\emph{stack pointer}),
and loads the context of the target thread.
This function $\mathsf{cswitch}$ can only be implemented at the assembly level,
and its specification does not satisfy the C calling convention.
To verify these low level scheduler functions, we introduce an
assembly-style concurrent layer interface $L_5$.
\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/scheduling.c}
\vspace{-10pt}
\caption{Pseudocode of scheduling implementation.}
\label{fig:exp:sched}
\vspace{-10pt}
\end{figure}


At layer $L_5$, we introduce three new events
$c.\yield$, $c.\sleep(i)$, and $c.\wakeup(i)$,
which map to $L_4$'s events
$c.\deq(\comm{pendq}(c))$,
$c.\enq(\comm{slpq}(i))$,
and $c.\enq(\comm{pendq}(c))$, respectively.
Since the mapping is one-to-one, the transformation functions for
the log ($f_{l4}$) and environment context ($f_{\oracle4}$)
can be defined trivially.
The specifications of $\yield$ and $\sleep$
are defined as follows:
\begin{small}
\begin{mathpar}
\inferrule{
l_0 = l \cons \oracle (l) \\
\replay_{\comm{sched}} (l_0, c) = (tid, tdqp, tcbp) \\
a' = a\set{\comm{kctxt}(tid) : \regs[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]} \\
l' = l_0 \cons c.\yield/c.\sleep(i) \\
\replay_{\comm{sched}} (l', c) = (rtid, \any, \any) \\
\regs' = \regs\leftarrow a.\comm{kctxt}(rtid)  
}{
 \oracle, c\vdash_{\comm{Asm}}  \spec_{\yield/\sleep}([]/[i],\regs, m, a, l,\any, \regs',  m, a', l')
}
\end{mathpar}
\end{small}%
The abstract states $\comm{tid}$,
$\comm{tdqp}$, and $\comm{tcbp}$
are hidden at $L_5$, since they can always be reconstructed
by the replay function $\replay_{\comm{sched}}$ given the current log.
The replay function $\replay_{\comm{sched}}$
is defined inductively.
Here we only present the case for the $\sleep$ event:
\begin{small}
\begin{mathpar}
\inferrule{
\replay_{\comm{sched}} (l, c) = (tid, tdqp, tcbp) \\
\comm{tdqp} (\comm{rdq}(c)) = r\cons q \\
\comm{tdqp} (\comm{slpq}(i)) = q' \\
\comm{tdqp}'  = \comm{tdqp}
\set{\comm{rdq}(c): q}
\set{\comm{slpq}(i):q'\cons tid}\\
\comm{tcbp}'  = \comm{tcbp}
\set{tid: \comm{SLEEP}}
\set{r:\comm{RUN}}
}{
\replay_{\comm{sched}} (l \cons c.\sleep (i), c) = (r, tdqp', tcbp') 
}
\end{mathpar}
\end{small}%
\noindent\textbf{Step 2 (multi-threaded machine $\TAsm$)} 
The  layer interface $L_5$ is defined for the whole
set of threads for CPU $c$ and does not support thread-local reasoning.
Ideally, we would like to reason about each thread running on the CPU 
locally, and later formally combine the reasoning to obtain a global
property for the full set of threads.
To support this, we need a machine model that gives semantics to
a partially-composed set of threads.

Let $T_c$ denote the whole set of threads running over CPU $c$.
From $\LAsm(L_5(c,\oracle))$, we construct a new 
multi-threaded \emph{partial} machine $\TAsm$,
and instantiate it with a layer interface $L_6$.
The result machine 
$\TAsm(L_6(c), A)\langle \oracle \rangle$ is 
parameterized over an active thread set $A \subseteq T_c$,
and can be instantiated into a regular machine for
an environment context $\oracle$.
The machine state of $\TAsm$ is defined as $s:=(\tid, f_\regs, m, f_a, l)$.
Here, $\tid$ is the current thread ID, and
$f_\regs$ and $f_a$ are partial functions
from IDs of threads in $A$ to a register set $\regs$
and a per-thread abstract state $a$, respectively (\ie,
we divide the register set and the abstract state
into separate ones for each thread).
The relation between abstract states
can be easily defined, but the relation of the register set between $L_5$ and $L_6$ is non-trivial.
For the currently-running thread, its local register set ($s_6.f_\regs(s_6.\tid)$) is equal
to the register set of $L_5$ ($s_5.\regs$).
For other threads of $L_6$, the local register set is equal
to the corresponding saved kernel context of $s_5$ for the registers
$[\comm{ra}, \comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]$.
In other words, the register set is decomposed in a way such that the $\regs$ of the
currently-running thread is equal to the machine's register set, while the $\regs$ of
other threads is equal to the corresponding kernel context saved by context switch.

\ignore{
However, $m$ is using
the CompCert memory model \cite{leroy08},
which is not compositional \ronghui{reference here?Tahina?}.
We introduce a new \emph{algebraic memory
model} to make the CompCert memory model
modular (\cf Sec.~\ref{sec:comp} for more details).
Two machines are composable only if they have the same
logical log $l$.
}

For instructions and primitives that do not change the current thread
ID, their semantics are lifted from $L_5$ to $L_6$
by instead operating on the state $(f_\regs(\tid), m, f_a(\tid), l)$.
The scheduling functions (which may change the thread ID to a thread outside of $A$)
are part of the definition of $\TAsm$
and may trigger environment steps by generating events
that will switch control to a thread outside of $A$.
For instance,
the $\yield$ operation of $\TAsm$
will generate a $c.\yield$ event that will switch control to the next thread
and set the machine's $\tid$ accordingly.
If the new thread is in $A$,
then the $\TAsm$ machine will simply keep running local steps of the new thread.
If the new thread is outside of $A$,
then the machine will take environment steps
until the log indicates that control switches back to  $A$.

When $A$ is the entire thread set $T_c$ of core $c$,
it will never switch to environment threads and
we have the following refinement:
\begin{lemma}{\small
$\forall \oracle\ ,~
\LAsm(L_5(c,\oracle)) 
\refines
\TAsm(L_6(c), T_c)\langle \oracle \rangle$}%
\label{thread_composition}
\end{lemma}

However,
the partial machine $\TAsm$ can also be decomposed using the linking operator $\Join$,
which enables the thread-local reasoning:
\begin{lemma}
{\small
$
\TAsm(L_6(c), T_c)
\Refrel_\id
\bigJoin_{t \in T_c}
\TAsm(L_6(c), \{t\})
$}
\label{thread_compose}
\end{lemma}

%-----------------------------
\ignore{
For scheduling functions (which may change the thread ID to one for a thread not in $A$),
it may trigger the environment step by querying the environment
context $\oracle_T$, which capture the behavior of \emph{inactive} threads (\ie, the threads in $T_c - A$).

We define the specifications of scheduling functions using $\oracle_T$.
For example, the $\yield$ primitive at $L_6$ is specified as:
\begin{small}
\begin{mathpar}
\inferrule{
l_0 = l \cons \oracle (l) \\
(l', \tid') =  \comm{yield\_back}
(A, \oracle_T, l_0 \cons c.\yield) \\
f_\regs' = f_\regs\set{\tid': 
\text{undefined value except for }[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]}
}{
 A, \oracle_T, \oracle, c\vdash_{\comm{Asm}}  \spec_{\yield'}([],\tid,f_\regs, m, f_a, l,\any, \tid', f_\regs',  m, f_a, l')
}
\end{mathpar}
\end{small}%

Note that $\yield$ changes the $\tid$ to $\tid'$.
After that, the execution operates on
the state $f_\regs'(\tid')$ and $f_a(\tid')$.
When $\tid'$ is different from $\tid$, this simulates the behavior of context switch
within the active thread set $A$. 
If the active thread set $A$ is $T_c$, we have:
}

\ignore{
If the active thread set $A$ is the whole
thread set $T_c$, we have:

Then, we can prove the simulation
relation:
\begin{lemma}[$\LAsm$ refines $\TAsm$]
\label{thread_composition}
{\small
\[
\forall \oracle,~
\LAsm(L_5(c,\oracle)) 
\refines
\TAsm(L_6(c,\oracle, T_c, \any))
\]}%
\end{lemma}



\begin{lemma}[CPU-local machine refines multi-thread machine]
{\small
\[
\forall M~ \oracle, 
\sem{ }{M}L_5(c,\oracle) \Refrel_{R_5} 
\sem{}{M}L_6(T_c, \oracle)
\]}
\label{whole_thread_composition}
\end{lemma}}

\ignore{
\textbf{Proof sketch}:
For whole thread set,
the environment context only carries
the information of the context CPUs.
Thus, $\oracle_{T_c} = \oracle$.
For the simulation relation $R_5(s_5,s_6)$, 
the only non-trivial part is the relation
for register set function $s_6.\regs p$.
For the current thread,
$s_6.\regs p(s_6.\tid)= s_5.\regs$.
For other threads,
the register set of $s_6$ is equal
to the kernel context of $s_5$
for the register list
$[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]$.
\ignore{
which is defined as:
1) ;
and 2)$\forall i \neq s_6.\tid,
s_6.\regs p(i)= s_5.a.\comm{kctxt}$.}
}
%-----------------------------

\noindent\textbf{Step 3 (thread-local layer interface $L_7$)} 
Finally, we introduce a per-thread layer interface $L_7$, which is parameterized
over a single active thread $t\in T_c$.
Thus, $\yield$ always yields back to itself,
and there is no longer any ``internal'' low level context switch among active threads.
Thus, we obtain the following big-step specification of $\yield$ that actually
satisfies the C calling convention:
\begin{small}
\begin{mathpar}
\inferrule{
l_0 = l \cons \oracle (l)\\
(l', \tid) =  \comm{yield\_back}
(\tid, \oracle, l_0 \cons c.\yield)
}{
 \tid, \oracle, c, \oracle \vdash  \spec_{\cyield}([],\regs, m, a, l,\any, \regs,  m, a, l')
}
\end{mathpar}
\end{small}%
Here, the auxiliary function $\comm{yield\_back}$ 
specifies the behavior of repeatedly querying the environment context
$\oracle$ until the control flow yields back to the thread of interest $\tid$.
It appends all the events triggered by the inactive threads to the log.
To prove termination-sensitive contextual refinement,
we prove that $\comm{yield\_back}$ actually terminates,
by showing that the scheduler is \emph{fair} and every running
thread gives up the CPU within a finite number of steps.

On top of this thread-local machine $\LAsm(L_7(c, t, \oracle))$,
we can reason about a thread's program 
locally at the C level by building per-thread layers.
We show that $L_6$ for a single thread is refined $L_7$:
{\small
\[ \TAsm(L_6(c), \{t\})\langle \oracle \rangle \Refrel \LAsm(L_7(c, t, \oracle)) \]
}%

% ------------------------------------------------------------------
% Let's not talk about linking partial machines here
\ignore{
By Lemma~\ref{lemma:mono},
per-thread layers can be composed using
``$\Join$", and propagated down to the $L_6$ machine.
Once the layers are composed for the full thread set $T_c$, we can apply Lemma~\ref{thread_composition}
to transfer the guarantee down to $L_5$
and link with per-CPU layers.
}
% ------------------------------------------------------------------

\paragraph{Queuing Lock} is an algorithm whereby waiting threads are put to sleep,
so that busy waiting is avoided.
Verification of this complex locking algorithm is particularly
challenging since its C implementation utilizes both
spinlocks and low level scheduler primitives ($\sleep$ and $\wakeup$).
In our framework, a queuing lock implementation is verified
by building multiple layer interfaces on top of $L_7$.

\ignore{
We first introduce two atomic operations (using $\comm{CAS}$)
that set and clear the busy bit of the lock.
Then we prove that the implementations
of the lock and unlock operations
satisfy their atomic specifications
(which generate single $\comm{acq\_q}$ and $\comm{rel\_q}$ events, respectively).
This is achieved by proving a simulation between layer interfaces together
with starvation-freedom, using a strategy similar to verification of
the ticket lock implementation.
}

\ignore{

 is a general
lock algorithm that 
allows sleeping for the lock, instead of busy waiting.
Since queuing lock
relies on the spinlocks and thread scheduling
and is implemented at C-level
(\cf Fig.~\ref{fig:exp:queue_lock}),
it is hard to reason about
and has never been verified in previous works.

In our framework, queuing lock can be easily verified
based on $L_7$.
We first introduce two atomic operations
to set the busy bit if the lock is free (\ie,
$\comm{CAS\_qlock}$, implemented
using $\comm{CAS}$),
and to clear the busy bit (\ie, $\comm{clear\_qlock}$).
Then, we prove the implementation 
of lock and unlock operations
satisfy the atomic specifications
(\ie, $\comm{acq\_q}$ and $\comm{rel\_q}$ events)
by showing the simulation relation
and the starvation-freedom.
The verification details are similar to the ticket lock.

\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/queue_lock.c}
\vspace{-5pt}
\caption{Pseudocode of queuing lock implementation.}
\label{fig:exp:queue_lock}
\vspace{-10pt}
\end{figure}

}
