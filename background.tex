% !TEX root =  main.tex
%
% BACKGROUND AND RELATED WORK
%
\chapter{Background and Related Work}
\label{chap:background}
%
The DOMP project contributes both a user-level scheme for ensuring parallel determinism and, within that scheme, a new extended reduction feature.  In this chapter, we review both the research context for the overall project of deterministic parallelism and that of the more specific reduction construct.  Following this, we review areas of research closely connected to the DOMP project, including other solutions to the problem of nondeterministic parallel computing.  These research areas fall into several categories:
\begin{itemize}
\item Programming languages
\item Record and replay systems
\item Debugging tools
\item Transactional memory systems
\end{itemize}
First, however, we place DOMP in general, and extended reductions in particular, in a conceptual context.

\section{Deterministic Parallel Computing}
\label{sec:background-determ}
%
A parallel program is \textit{deterministic} if the input alone determines the output, regardless of extrinsic events such as the OS's thread scheduling.  
The contrary of this condition is \textit{nondeterminism}.  Note that nondeterminism is distinct from underspecification.  A programming language or library, for instance, may leave some details unspecified, and yet the system may execute the program deterministically, in the sense that the same input always results in the same output and behavior.  In such a case, some other elements of the runtime system, ``downstream'' from the program, are sufficiently specified so as to fill in the semantic gaps that the program has left indeterminate.   We call an execution \textit{nondeterministic} if the output or behavior depend in some way on events, such as thread scheduling or hardware timing, over which the program has no control, and that no programmer would be able to predict based on the program and the input.  Underspecification in a system allows such nondeterminism to occur.

If any two threads have access to the same location in shared memory, and at least one such access is a write, the definite ordering of such accesses is essential.  Such a condition is a \textit{conflict} in accesses, which may be either a read-write conflict (one thread reads from the same location that another thread updates) or a write-write conflict (two threads write different values to the same location).  A lack of order on conflicting accesses, where the sequence of accesses and data visibility changes differs from one run to the next, is a prime manifestation of nondeterminism in the system.  We call this condition a \textit{data race}.  In a data race, the reading thread may get either the old or the updated value, depending on timing or scheduling factors not prescribed in the program.  Thus any data race is likely to cause a heisenbug, since it will likely result in different outcomes on different runs, at least one of which is unwanted and erroneous, but, by the same token, not reliably reproducible.    

By itself, the above definition of determinism allows for a range of behaviors, depending on the \textit{synchronization} and \textit{memory consistency models}.  

Synchronization is \textit{naturally} deterministic if program logic alone determines how and at what points different threads interact, depending only on computation state and not on timing.  
A \textit{fork} deterministically creates a child thread at a program-defined point in the parent's execution, for example.  Similarly, a \textit{join} deterministically combines the parent's and child's flows at program-defined points in {\em both} threads.  Other common constructs, such as mutex locks, condition variables, semaphores, monitors, and OpenMP's \textit{atomic}, \textit{critical}, and \textit{flush}, are semantically nondeterministic: they allow a thread to signal or wake up an {\em unspecified}, nondeterministic recipient---e.g., the next holder of a lock---or to wait for an event from a nondeterministic source---e.g., any of several threads that might signal a condition variable.  The choice of the recipient or source thread in such cases depends on the runtime scheduler or some other agent, not on what the program itself specifies.

In a similar way, a memory consistency model is deterministic if program logic alone determines the order in which threads update shared memory and in which each thread sees such updates.   A naturally deterministic memory consistency model precludes data races, since it by definition orders all memory accesses to shared memory in which any done thread can affect another.

Classic memory consistency models, including sequential 
consistency~\cite{lamport79multi,sezgin05sequential} and relaxed models~\cite{gharachorloo90memory, saraswat07memmodels}, introduce nondeterminism, by leaving memory access interleavings underspecified.  That is, even if a program uses only deterministic synchronization abstractions (e.g., fork/join) and runs on sequentially consistent hardware, data races and execution timing can make the program exhibit any one of an exponentially large variety of sequentially consistent memory access interleavings.

A sequentially consistent program, for instance, meets the following requirements~\cite{lamport79multi}:
\begin{itemize}
\item ``The result of any execution is the same as if the [memory] operations of all the processors [or threads] were executed in some sequential order,'' implying that every thread in the program sees the same updates to shared memory in the same order (but not at all necessarily at the same time)
\item The order in which every thread sees these shared memory updates corresponds to the order in which the program specifies them.
\end{itemize}
Thus if the program has Thread A update $x$ twice, first to the value $1$ and to the value $42$, Thread B reads $x$ twice, but its first read operation returns $42$, the system violates sequential consistency.  Now suppose that the system is indeed sequentially consistent (so that Thread B reads $1$ first).  After these operations, both Thread A and Thread B increment $x$ (whose current value is $42$).  Suppose that, in their respective non-atomic increment operations, both threads read $x$ before either one writes to $x$.  Each thread sees the same sequence of updates, i.e., from $42$ to $43$ (erroneously), and thus they do not violate sequential consistency.  The same applies if Thread A writes $43$ back to $x$ before Thread B reads $x$:  in this case, both threads see the same sequence of two updates to $x$.  Thus the classic example of a data race can easily occur under in a sequentially consistent system.

Weaker memory consistency models impose more constraints on the \textit{synchronization} operations---here, the fork before and the join after any of the memory accesses mentioned---but even fewer constraints on other operations.  Thus the same data race could occur \textit{a fortiori}.

Transactional memory~\cite{harris10tm}, whether implemented in 
software~\cite{dolev08carSTM,	herlihy03stm} or in
hardware~\cite{herlihy93transactional}, may assume that the underlying memory architecture implements either sequential consistency or a more relaxed memory consistency model.  Transactional memory
ensures the \textit{atomicity} of groups of memory accesses within a transaction, and therefore prevents data races within the transaction unit.  It does not, however, specify the order of transactions among threads, but leaves this specification to the programmer, much as do low-level synchronization primitives such as mutex locks and condition variables.  By itself, then, transactional memory effectively allows higher-level data races~\cite{artho03high} at the larger scale of transactions, though it prevents them at the level of particular read or write accesses.

By contrast, workspace consistency~\cite{aviram11workspace} requires that every write to shared memory be associated with the ensuing read or reads according to a prescribed pattern that the programmer could therefore deduce by examining the program, with knowledge of the ordering rules of the system.  We examine workspace consistency further in~\ref{sec:domp-wcd}.

Several research efforts have focused on running a nondeterministic program, perhaps containing data races, deterministically.  Deterministic schedulers such as DMP~\cite{devietti09dmp} and CoreDet~\cite{bergan10coredet} execute a semantically nondeterministic program repeatably, by artificially synthesizing {\em one particular} (arbitrary) interleaving of the program's synchronization and memory access events.  Deterministic scheduling can reproduce races once detected, but it neither eliminates races nor guarantees that they {\em will} be detected.  A program's behavior may still depend on the (deterministic) execution schedule in subtle ways not explicit in program logic, as in this example:
%
\begin{small}
\begin{verbatim}
// Thread A:
{
  if (input_is_typical)
    do_a_lot();
  x++;
}
// Thread B: 
{
  do_a_little();
  x++;
}    
\end{verbatim}
\end{small}
%
Under ``typical'' program inputs, abstracted here via `\verb|input_is_typical|', a deterministic scheduler may always cause thread $B$ to reach its increment of $x$ while thread $A$ is executing its long-running and non-conflicting \verb|do_a_lot()|.  But some particular ``rare'' input, which unit tests may not have covered, may cause the threads' increments to line up in the deterministic execution schedule, resulting in a classic data race and an ``input-dependent heisenbug.''

Another scheduling approach, Kendo~\cite{olszewski09kendo}, enforces deterministic synchronization by ordering lock acquisitions and releases, but does not constrain memory accesses, so that programs containing data races (through a failure to lock) will run nondeterministically.

Grace~\cite{berger09grace} is a deterministic scheduler that emulates sequential consistency by means of speculative execution and transactional memory techniques.  Determinator~\cite{ford10efficient} and Revisions~\cite{burckhardt10revisions} avoid the complexity of speculative execution by straying from sequential consistency.  These projects achieve acceptable overhead for some workloads, but constrain programs to a minimal set of deterministic synchronization primitives such as \textit{fork/join} and \textit{barrier}.  Revisions, moreover, a C\# library, does not directly support legacy code written in C-like languages.

Another deterministic scheduling system that avoids both the input sensitivity described above for quantum-based systems and the overhead of speculative execution and rollback is Tern~\cite{cui10stable}, which \textit{memoizes} execution schedules for reuse.  This solution, like all deterministic schedulers, solves a related but distinct problem from the one that both Determinator and DOMP solve:  the former allow racy programs to run reproducibly, while the latter eliminate data races by enforcing a deterministic programming model.

Aside from deterministic scheduling, record and replay systems, such as Recap~\cite{pan88supporting}, Instant Replay~\cite{leblanc87debugging}, DejaVu~\cite{choi98deterministic}, and ReVirt~\cite{dunlap02revirt}, as well as many others, at least enable the user to reproduce any bug on demand.  These systems, however, generally impose too much overhead to work feasibly on deployment systems, or else require special hardware.

Deterministic parallel \textit{functional} programming languages
have long been available~\cite{hudak86parafunctional,roe91parallel}, including dataflow
languages~\cite{ackerman82dataflow,faustini82dataflow}
and parallel Haskell~\cite{aditya95ph,chakravarty07parallelHaskell,
jones93implicitExplicit,trinder02parallel}.  Following in the dataflow tradition
is the recent Concurrency Collections (CnC) language~\cite{budimlic10cnc},
which specifies the parallel
execution of code ``kernels'' (routines), which may, in turn, be written in any of a wide range of languages, including conventional imperative ones such as C++ or Java.  All of these languages derive their determinism from the ``single assignment rule'':  a variable is undefined until a thread defines it with a value, at which point it is also immutable.  In this way, data races are impossible.  Programmers have been slow to adopt the functional language paradigm, however, and CnC, which accommodates conventional programming for the (serial) ``kernels'', presupposes a division of labor between the ``domain experts'' who develop the ``kernels'' and the ``tuning experts'' who parallelize it using CnC's relatively unfamiliar notation and concepts.

Some recent languages support deterministic parallelism while allowing for a conventional imperative style. The SHIM language~\cite{edwards06shim, edwards08programming, tardieu06scheduling} implements a deterministic message passing model, avoiding the challenges of making shared memory deterministic, but also foregoing the programming convenience of the shared memory abstraction and requiring programmers to marshal data into explicit messages. Deterministic Parallel Java~\cite{bocchino09dpj} offers shared memory, but requires the programmer to adopt a new Java type system, and to ``prove'' statically via typing rules that parallel code is race-free.  Array Building Blocks~\cite{ghuloum10arbb} promise deterministic parallelism, but their proprietary nature hampers detailed inspection.

DOMP has many architectural similarities to Dthreads~\cite{liu11dthreads}.  Like Grace and DOMP, the Dthreads scheme supports programs in C-like languages, written using a standard parallel API.  Like Determinator and DOMP, Dthreads do not require speculative execution and rollback, and therefore achieve good performance.  Dthreads' efficiency, however, depends on their imposition of an arbitrary shared memory commit order---a form of deterministic scheduling---which allows racy programs to execute deterministically.  By contrast, DOMP offers a way to allow conventional programs to conform, as much as possible, to a purely deterministic programming model, which treats data races as errors.

DOMP's approach has most in common with that of Determinator~\cite{ford10efficient} and Revisions~\cite{burckhardt10revisions}, operating in the workspace consistency model~\cite{aviram11workspace}, while supporting a wider range of naturally deterministic synchronization abstractions to increase compatibility with legacy code.

\section{The Reduction Construct}
\label{sec:background-reductions}
%
In addition to supporting OpenMP's core features, DOMP offers a generalized reduction construct.  A reduction (or \textit{right fold} in functional programming) is a higher-order function that applies a binary \textit{combining operation} first to an initial value and the first element of a list, and then iteratively to the results of the previous iteration and the next element of the list, until every element has been consumed.  For example, if the list contains integers and the combining operation is addition, a reduction over the list will give the sum of the integers. In parallel programming, each ``element'' of the ``list'' is a single thread's instance of a shared variable; reduction aggregates the values of this variable across threads according to the combining operation.

The standard OpenMP reduction construct takes the form of a clause modifying the OpenMP directive that stands at the head of a structured block and specifies that the ensuing block be executed in parallel.  Within the block, the variable (or variables) listed in the clause will be \textit{updated} using conventional updating syntax as in sequential programming, as in the following example:
%
\begin{verbatim}
int x = 0;
#pragma omp parallel reduction(+:x)
  {
    x += 42;
  }   // x == 42 * num_threads
\end{verbatim}
%
This reduction has the special semantic feature that it produces the same results even if the program disregards the OpenMP directives, e.g., if the programmer compiles without OpenMP support.  (With GCC, this means compiling without the \texttt{-fopenmp} flag.)  Unfortunately, OpenMP's reduction construct only supports a handful of arithmetic, bitwise, and logical operations, and only scalar, value types.   This means that such simple operations as vector addition and matrix multiplication, as well as more complex and algorithm-specific ones, are unsupported.  Furthermore, the OpenMP specification does not stipulate an evaluation order.  In general, implementations such as the one in GOMP evaluate the reduction in a nondeterministic order, using low- or machine-level mutex locks.

Before and outside of OpenMP, reductions have a long history in programming language theory~\cite{iverson62apl}, and are a key concept in a theoretical understanding of parallel programming in a functional programming language~\cite{skillicorn05foundations}.  Thus DOMP's extended reduction is not a new concept, but its conformity to OpenMP reduction semantics, and its use in bridging the gap between conventional parallel programming and a purely deterministic programming model, are among its contributions.

A wide range of modern programming languages, both functional and imperative, support reductions in sequential programs in one way or another, including C\#, C++, D, Haskell, JavaScript, Lisp, ML, Perl, PHP, and Python.  The C++ \texttt{std::accumulate} library routine, for instance, allows the programmer to reduce over an iterable sequence, using an initial value and an arbitrary combining operation---or sum by default, if the programmer supplies no operation (see pp. 682 -- 3 in~\cite{stroustrup97c++}):
\begin{verbatim}
template <class In, class T, class BinOp> 
T accumulate(In first, In last, T init, BinOp op) {
  while (first != last) init = op(*first++);
}
\end{verbatim}
This amounts to mere syntactic sugar for iterating over the sequence and repeatedly applying the operation, since it implies no parallel execution and cannot be readily adapted to parallel programming.

The MapReduce algorithm's second, \textit{reduce}, phase applies a reduction across large lists of data entries in the nodes of a distributed system\cite{dean08mapreduce}.  First, the \textit{map} phase, running in parallel on many nodes, takes the input and returns a set of key-value pairs.  In the \textit{shuffle} phase, the system redistributes the key-value pairs among the \textit{reducer} nodes, so that each node has a subset, or list, of such key-value pairs to reduce.  Like the mappers, the reducer nodes work independently and in parallel, although the MapReduce algorithm does not require parallelism within the reducer node itself.  Moreover, the algorithm is not embedded within a larger scheme of parallel execution as is the OpenMP reduction.  The Phoenix project~\cite{ranger07mapreduce} ports MapReduce to the multicore platform, but, likewise, launches and terminates parallel execution for the special purpose of the MapReduce problem alone.  Neither the original MapReduce nor Phoenix defines a deterministic order for evaluating the reduction.

CnC has a proposed new reduction construct~\cite{budimlic11deterministic}, which, like the
rest of CnC, is provably deterministic~\cite{budimlic10cnc}.  This reduction works with arbitrary combining operations but requires the same unfamiliar notation and programming paradigm as the rest of CnC.

Intel's Threading Building Blocks C++ library (TBB)~\cite{intel12tbb} defines a \texttt{parallel\_reduce} template function, bearing some similarities to C++ \texttt{std::accumulate}, but providing for parallel execution of the reduction.  Like the latter and unlike OpenMP's reduction clause, \texttt{parallel\_reduce} allows for arbitrary types and operations. Its data argument is an object of the \texttt{Range} template class, which is more general than the iterable object supplied to \texttt{std::accumulate}; having instead a \texttt{split} function, to split the data into two parts, recursively.  Like Phoenix and unlike OpenMP's reduction clause, this reduction spawns and terminates its own parallel team of threads rather than working within a larger parallel block, although TBB does support nested parallelism.  TBB's \texttt{parallel\_reduce} recursively splits the data as far as possible and then applies the combining operation to pairs of data entries along a binary tree in the process of joining threads---a pattern similar to the workings of DOMP's extended reduction, as further explained in~\ref{sec:reduction-api} 
and~\ref{sec:impl-xreductions}.  However, in contrast to DOMP, TBB's order of splitting the data and thus the shape of the resulting evaluation tree are explicitly nondeterministic.

\parasep

The design of DOMP should be seen within the broader context of various relevant areas of research.  We turn to these next.

\section{Programming Languages}
The functional programming language research community has been interested in parallelism at least since the 1980s~\cite{peytonjones89parallel}.  Parallelism and functional programming make a good fit because functional programming languages can express computations guaranteed to be devoid of side effects.  (Languages such as Haskell and ML do provide special features to express side effects separately from pure functions, e.g., \textit{monads} in Haskell and \textit{references} in ML.)  A program written in a \textit{purely functional} language, devoid of side effects, or in the purely functional subset of a functional language, is deterministic by nature, since it does not allow any interactions among concurrent processes:  the result of a parallel program will always be the same for a given input, and also the same as that of its sequential version~\cite{hammond94functional}.  Moreover, functional languages do not generally include an \textit{assignment} operation, since such an operation is defined as exerting a side effect upon the value of the left-hand operand.  For this reason, functional languages are also called ``single-assignment'' languages:  the definitional expression \texttt{let x = 42} means that $x$ will forever have the value $42$ in the given scope and no other value.  This ``single-assignment rule'' is another aspect of what makes functional languages deterministic when parallelized:  the concept of \textit{updating} a variable has no place here, so data races are impossible.

(The functional programming language ML has a parallel version, CML~\cite{reppy93cml}.  However, CML seems to have been developed specifically to make certain kinds of nondeterminism possible in interactive systems.)

Early research focused primarily on \textit{implicit parallelism}, in which the compiler could easily infer from the function what elements of it could be computed independently and therefore in parallel, as in Id~\cite{nikhil93id}, pH (a parallel dialect of Haskell)~\cite{aditya95ph}, and pHluid~\cite{flanagan96phluid}.  This focus continues with Manticore~\cite{fluet10manticore}.  At the same time, Paul Hudak developed a system of \textit{annotations} whereby a programmer could nudge the compiler to parallelize particular regions of code and in what configuration~\cite{hudak86parafunctional}, which made better performance possible for a wider range of programs, while at the same time separating off parallelization from core algorithmic concerns~\cite{trinder98algorithm}.

Dataflow languages are functional programming languages developed specifically for dataflow architectures~\cite{whiting94dataflow}, which are machine architectures designed to exploit parallelism by circumventing the bottlenecks believed inherent in the von Neumann architecture~\cite{arvind1990executing,johnston04dataflow}.  Moreover, their determinism is evident in that they follow the model of the Kahn process network in the transfer of data among (physical or virtual) processors~\cite{faustini82dataflow}.  Dataflow languages are compiled into \textit{dataflow graphs}, which the system uses to control the flow of data.  Languages such as Cajole~\cite{hankin81cajole}, Lucid~\cite{wadge85lucid}, and LUSTRE~\cite{halbwachs91lustre} are textually based and treat the data flow graph as an internal representation.  But other dataflow languages have visual external representations for the benefit of clarity and ease in software engineering.  These include the commercial products LabVIEW~\cite{labview12} and, more recently, Simulink\cite{simulink12}, which is widely used today for the design of control systems.  These languages can run on any modern platform, but are specialized in their purposes.  As a whole, dataflow languages are clearly both deterministic and useful, but limited with respect to use cases.

A number of more conventionally imperative-style languages have emerged in recent years that offer deterministic parallelism.  SHIM~\cite{edwards06shim, tardieu06scheduling, edwards08programming} is a deterministic parallel programming language with C-like syntax and in the imperative style, whose parallelism follows the message-passing paradigm.  It was developed particularly for the design of embedded systems.   Deterministic Parallel Java (DPJ)~\cite{bocchino09dpj} uses shared-memory parallelism, but requires the programmer to annotate data according to a type and effect system that determines which data are visible and otherwise accessible to which threads at various points in the program.  Guava~\cite{bacon00guava}, presented as a ``dialect of Java without data races,'' uses a comparable annotation system, and restricts programs so as to allow concurrent threads to have access to data only if such accesses are properly synchronized.  Although this principle might at first appear to eliminate data races, it seems to us to leave open the possibility of the ``higher-level data races''~\cite{artho03high} mentioned in \ref{sec: background-determ} with regard to transactional memory.

Like DPJ and Guava, Jade~\cite{rinard98jade} involves the annotation of data to make access permissions explicit, but uses this information to parallelize serial code automatically, working with an original source in a conventional language such as C.  Jade's promise is to preserve sequential semantics in parallel execution, something to which standard OpenMP aspires in principle, but without the latter's excluding nondeterminism.  Another programming language based on C and providing for parallelism through annotations is Cilk~\cite{blumofe95cilk,leiserson97cilk,frigo98cilk}, which also strives to provide the same semantics under sequential and parallel execution, but which, like standard OpenMP, does not actually prevent nondeterminism.

\section{Deterministic Scheduling}
The approach of imposing a deterministic schedule on a possibly nondeterministic and even racy or buggy program has interested many research groups.  DMP~\cite{devietti09dmp} and CoreDet~\cite{bergan10coredet} divide program ``time'' into uniform units, or \textit{quanta}, consisting of a fixed number of instructions, with each quantum divided between an initial parallel phase and a sequential phase.   Through static analysis, the compiler detects and marks potentially conflicting accesses to shared memory.  The runtime system shifts from parallel to sequential execution as soon as it encounters the first such conflicting instruction within a quantum.  Figure~\ref{fig:background-ds-quanta} illustrates this scheme.  In the sequential portion, of course, the runtime imposes an arbitrary but consistent order on threads' accesses to shared memory, which ensures the deterministic outcome of the program.
\begin{figure}
\centering
\includegraphics[width=1\textwidth]{ds_quanta.eps}
\caption{Execution quantum scheme in deterministic schedulers such as DMP and CoreDet.}
\label{fig:background-ds-quanta}
\end{figure}

Grace~\cite{berger09grace} does not follow the same quantizing model as DMP and CoreDet, and, indeed, it shares many elements with DOMP:  it implements threads as separate processes, each working within its own address space; it tracks writes (and also reads) using access protection and a signal handler; it eliminates mutex locks, converting those that occur in source code into no-ops.  Grace threads write their updates to shared memory to local copies, as in DOMP, and the runtime \textit{commits} those updates as transactions at synchronization points, which are any points where threads fork or join.  A major difference from DOMP, however, is that Grace threads update shared memory \textit{speculatively}:  at synchronization points, they attempt to commit, but if a thread's attempt has the wrong version number, kept in a global data structure, execution aborts and restarts the commit sequence.  The resulting sequence of changes to shared state is always the same, and therefore deterministic, but aborted and restarted transactions could waste resources.  Dthreads~\cite{liu11dthreads} combines ideas such as threads-as-processes for thread isolation and protection and trapping to track writes from Grace with the quanta divided into parallel and sequential phases and a deterministic ``write token'' that orders writes to shared memory in the sequential phase from DMP and CoreDet.  Dthreads also achieves remarkable efficiency by means of tricks such as minimizing writes by ``diffing'' them against the original state, a technique borrowed from TreadMarks~\cite{amza96treadmarks} and Munin~\cite{bennett90munin}.  It also minimizes false sharing, resulting in performances sometimes better than those of the reference \texttt{pthreads} implementation.   And unlike both Grace and DOMP, Dthreads supports parallel programs that do not conform to the simple fork/join model.

Kendo, like DMP, CoreDet, and Grace, uses performance counters to compute a deterministic schedule; but unlike these solutions, it concentrates exclusively on the deterministic ordering of lock acquisitions.  It therefore does not have to quantize execution and is both simpler and more efficient than DMP or CoreDet.   However, a program that has a data race---arising from a failure to use mutual exclusion locks where necessary---will remain racy and nondeterministic under Kendo.  Thus Kendo can only guarantee that a race-free program will always execute with the same schedule, a guarantee of some usefulness, especially in Byzantine fault tolerance and other systems that require exact replication of computations.

Revisions~\cite{burckhardt10revisions} follows a model very close to that of DOMP:  threads are processes, each one working in its own, isolated copy of shared state.  However, Revisions resolves memory access conflicts according to the \textit{isolation type} of each conflicting object, rather than, as in DOMP, by signaling a race condition error.  This approach can only work in a strongly typed language that can record additional type information---and, of course, it does not assume a deterministic programming model, but rather the deterministic execution of a strongly typed programming model.

\section{Record and Replay Systems}
An approach to deterministic parallelism entirely different from programming languages or deterministic schedulers is to have the system monitor the execution of a parallel program, log every event important to parallel execution, and then make it possible to replay the execution, using the log, with the exact same interleaving, resulting in the same output and behavior.  This approach is called 
record 
and replay, and it can be of great use in debugging parallel code that may 
contain heisenbugs~\cite{agrawal91backtracking, king05debugging, srinavasan04flashback}, as well as in intrusion detection~\cite{dunlap02revirt}.  Record and replay systems present a difficult trade-off between cost and performance.  Software record and replay systems, such as DejaVu~\cite{choi98deterministic}, ReVirt~\cite{dunlap02revirt}, and Doubleplay~\cite{veeraraghavan11doubleplay} are inexpensive, but tend to slow application performance down to the point where it would not be feasible to run them continually on high-use deployment systems such as Web servers.  By contrast, systems including special hardware, such as Karma~\cite{basu11karma} and FDR~\cite{xu03replay} 
are very efficient but expensive.  Research continues in this area to develop a record and replay system that is at once fast and affordable.

\section{Debugging Tools}
A number of debugging tools and systems are available to help programmers to cope with nondeterminism in their parallel programs.  Cilk has its own special debugging tool to help catch data races~\cite{leiserson97cilk}.  For more general purposes, RacerX~\cite{engler03racerx} can find concurrency bugs, both data races and deadlocks, through static analysis.  The Valgrind project's concurrency bug detector Helgrind also works through static analysis---in particular, by building a graph of ``happens-before'' relations~\cite{nethercote07valgrind}.  In our own, admittedly slight experience with Helgrind, however, we found it to produce so many false positive results as to be of limited usefulness.  Eraser~\cite{savage97eraser} detects concurrency bugs dynamically, by monitoring all shared memory references, making it somewhat resemble record and replay systems.

\section{Transactional Memory}
As mentioned in \ref{sec:background-determ}, transactional memory ensures atomicity of updates to shared memory but does not, in and of itself, prevent data races in the program.  However, transactional memory shares some techniques with DOMP, and provides a partial model for addressing some of DOMP's central issues:  thread isolation and the minimization of synchronization overhead.  Herlihy and Moss's hardware transactional memory~\cite{herlihy93tm}, for example, uses a slight modification of the standard ``snoopy'' cache coherence protocol to enable the cache to detect transaction conflicts.  In so doing, it minimizes synchronization overhead, essentially to setting or clearing bits in the cache directory.  This design takes care to allow normal memory accesses to proceed without interference, as if isolated from the transactions.  Moreover, both this system and Shavit and Touitou's software transactional memory (STM)~\cite{shavit97software}, whose design is based on the former, avoid locks and blocking entirely.  These two and other STM systems generally assume that multiple \textit{processes} will be sharing the same transactional memory but not the same address space, i.e., that they work essentially in isolation except when interacting through a transaction~\cite{dolev08carSTM, herlihy03stm, harris10tm}.  This model, in which the memory serves as a lock-free synchronization manager, resembles the role of the runtime library in DOMP.  However, DOMP does not resolve or retry conflicting memory accesses, but rather signals an error in the program in such cases.  

\parasep

Among deterministic parallel programming solutions that support C-style languages, Determinator and DOMP are unusual in requiring that the program conform to a deterministic programming model that does not allow data races and that precludes the use of such low-level synchronization primitives as muteness and condition variables, which assume an underlying nondeterministic model.  This approach raises a question:  how important or necessary are such nondeterministic language elements to real-world programming?  In order to approximate an answer to this question, we pursued the analysis that follows in Chapter~\ref{chap:analysis}.
