\chapter{Case Study: Building Certified Concurrent OS Kernels}
\label{chap:conkernel}

To demonstrate our concurrent framework,
we extend the \mCTOS{} single-core verified kernel into a
concurrent kernel \cCTOS{} running on multi-core hardware.
\ronghui{Fix the road map}

\section{Overview of the \cCTOS\ kernel}
\label{sec:con:overview}

\begin{figure}[t]\centering
\includegraphics[scale=.78]{figs/sysarch}
\caption{System architecture for the \cCTOS\ kernel.}
%\rule[0in]{\columnwidth}{.15mm}
\label{fig:sysarch}
\hrulefill
\end{figure}

Figure~\ref{fig:sysarch} shows the system architecture of the
\cCTOS\ kernel. The \cCTOS\ system was initially developed in the
context of a large DARPA-funded research project.  
\ronghui{Fix}
It is a concurrent
OS kernel that can also double as a hypervisor.  It runs on a military
land vehicle with a multicore Intel Core i7 machine. On top of the
\cCTOS\ kernel, we run 6 Ubuntu Linux systems as guests (one each on
the first six cores). Each virtual machine runs several RADL (The
Robot Architecture Definition Language) nodes that have fixed hardware
capabilities such as access to GPS, radar, \etc{}  The kernel also
contains a few simple device drivers (\eg, interrupt controllers,
serial and keyboard devices). More complex devices are either
supported at the user level, or directly passed through (via
IOMMU) to various guest Linux VMs. By running different RADL nodes in
different VMs, \cCTOS\ provides strong isolation support so that even
if attackers take control of one VM, they still cannot break into
other VMs to compromise the overall mission of the drone.
%\vilhelm{Maybe cut this paragraph, and drop the ``-like''? The figure does not mention 
%the VMs, GPS, etc, and the text creates the impression that 
%we ran the certified version on the vehicle. }

Within \cCTOS, we have various shared objects such as lock modules
(Ticket, MCS), sleep queues (SleepQ) for implementing queueing locks
and condition variables, message queues (MsgQ) for waking up a
thread on another CPU, container-based physical and virtual memory
management modules (Container, PMM, VMM), a Lib Mem module for
implementing shared-memory IPC, synchronization modules (FIFOBBQ,
CV), and an IPC module. Within each core (the purple box), we have
the per-CPU scheduler, the kernel-thread management module, the process
management module, and the virtualization module (VM Monitor). Each
kernel thread has its own TCB, context, and stack.

\paragraph{What have we proved?}
Using \CTOS{} framework, we have successfully built a fully certified version of
the \cCTOS\ kernel and proved its contextual refinement property with
respect to a high-level deep specification for \cCTOS.  This important
functional correctness property implies that all system calls and
traps will strictly follow the high-level specification and always run
{\em safely} and {\em terminate} eventually; and there will be no data
race, no code injection attacks, no buffer overflows, no null pointer
access, no integer overflow, \etc{}

\begin{figure}[t]
%\includegraphics[scale=0.1]{figs/layer_diagram.png}
\includegraphics[width=1.0\textwidth]{figs/layer_diagram.pdf}
\caption{Layer hierarchy of \cCTOS{} kernel}
\label{fig:layer_diagram}
\hrulefill
\end{figure}

\ignore{In addition to all the concurrent objects described in Sec. \ref{sec:prog},
the kernel also implements an MCS lock \cite{mcs91}, paging-based dynamically
allocated virtual memory,
a synchronous inter-process communication (IPC) protocol implemented using the
queuing lock, and a shared-memory IPC protocol with a shared page.
Using the techniques and strategies presented in the paper, we have
successfully specified and verified the \cCTOS{} kernel in the Coq proof assistant.}

Figure~\ref{fig:layer_diagram} shows the layer hierarchy of  \cCTOS{}.
The gray boxes denote machine models, on top of which the
per-CPU (blue) and per-thread (yellow) layers are built. Orange boxes
are user threads.

The bottom-left portion of the figure illustrates the contextual refinement
among machine models (\cf Section~\ref{sec:mach}), where we gradually turn the nondeterministic
multicore machine model with arbitrary interleavings
among different processors into a CPU-local machine model that is parameterized
over the behaviors of other processors,
where the switch points only appear at shared operations. This new machine
model allows us to reason about programs running on different processors locally,
and later compose them formally to reason about the whole program.

On top of this abstract machine model, the code of the concurrent kernel is specified and 
verified through 59 abstraction (logical) layers. For each CPU,
we introduce the atomic (ticket and MCS) spinlock objects
(\cf Section~\ref{sec:base:lock} and \ref{sec:con:mcs}).
On top of that, the device drivers running inside kernel are verified
(\cf Section~\ref{sec:con:device}).
Then there are multiple layers used to introduce memory management units (\cf Section~\ref{sec:con:mem}),
the thread context, atomic queue object (\cf Section~\ref{sec:con:queue}), and scheduler methods (\cf Section~\ref{sec:con:thread}).
The \texttt{PThread} layer is the topmost layer built for a particular processor.
There, the scheduler primitives like \texttt{yield} and \texttt{sleep}
are specified in a small-step manner, similar to how they are implemented
in C and assembly. Their specifications do not follow the C calling conventions
and thus cannot be called by C code. Above \texttt{PThread}, we then build up the 
per-thread layers that support thread-local reasoning for each CPU.
We first introduce the layer \texttt{PHThread} which defines big-step semantics for
the scheduler primitives that can be invoked from the C level (\cf Section~\ref{sec:con:thread}).
Finally, above the \texttt{PHThread} layer, we verify the
IPC and trap handler modules.

The verified kernel source code (both C and assembly) is extracted using 
Coq's extraction mechanism. The C source code is then compiled by the
extracted verified compiler and merged with the extracted assembly source to
produce the final assembly source code for our verified concurrent kernel. 


More importantly, because $\sem{\rm{}x86mc}{K\join{}P}$ refines
$\sem{\rm\cCTOS}{P}$ for any program $P$, we can also derive the
important {\em behavior equivalence} property for $P$, that is,
whatever behavior a user can deduce about $P$ based on the high-level
specification for the \cCTOS\ kernel $K$, the actual linked
system $K\join{}P$ running on the concrete x86mc machine would indeed
behave exactly the same.  All global properties proved at the
system-call specification level (including information-flow
security~\cite{costanzo16}) can be transferred down to the lowest
assembly machine.


\ignore{
Finally, we also proved that there is no stack overflow or memory
exhaustion in the kernel using recent techniques developed by
Carbonneaux~{\em et al}~\cite{veristack,ccrb15}.
\vilhelm{Maybe omit this paragraph, it draws attention to the fact that stack usage is 
  not tracked by CompCert. We mention it later in the text when talking about the Thread management module.}
}



\input{con-case/con-base}
%\input{con/con-prog}      % Layered Concurrent Programming (3-3.5 

\input{con/con-comp}      % Certified Compilation and Linking (1-1.5 pages)


\paragraph{Verification Effort and Lessons Learned}
Our team completed verification of the \cCTOS{} kernel in about 2 person years.
The layered approach is key to the scalability and feasibility of such a
large-scale verification effort.
One benefit of our approach is that
concrete and highly optimized (and thus complex) implementations can be abstracted
into much simpler logical specifications that are easier to reason about,
\eg, abstracting a queue implemented as a doubly-linked list into a simple logical list,
abstracting two-level page tables into a logical map from virtual addresses
to physical addresses with permissions, {\it etc}.
Besides this straightforward benefit, the layered approach shines in many other
aspects of complex system verification.

Layering allows us to perform incremental refinement of machine
  models. Real world CPUs are far from ideal for reasoning
purposes, so we abstract the realistic
machine model into a simpler one.
As indicated in Figure~\ref{fig:layer_diagram}, the per-CPU layers are built on top
of an abstract machine model that supports CPU-local reasoning. Through
multiple contextual refinements, we have proved that the underlying
nondeterministic hardware machine model refines the abstract model. Furthermore, 
because each layer in our framework
is an abstract machine, we can do this not just at
the bottom, but at any point within the verification stack. For example, 
during the \cCTOS{} verification, we first abstract the two-level page table structure
in memory into a logical map from virtual addresses to physical addresses
with permissions. After doing this, we \emph{then} abstract the low level machine 
model that implements page-based virtual memory as described in the hardware 
manual into a simpler model using our abstract logical map.

Layering eases code verification in the concurrent setting.
It allows us to separate code verification from logical reasoning.
Concurrent program verification can be treated as a process of building
verified atomic objects. Each method of an atomic object is implemented using
locks and other atomic objects introduced at lower layers.
When we verify the function body of such a method, we first simply treat it as 
if it were sequential; this results in a non-atomic specification that allows
multiple events to occur. Then, through a separate contextual refinement
that does not involve any code verification, we abstract the environment
context to obtain a new, atomic specification generating only a single event.
In this way, we manage to cleanly separate concurrency reasoning 
(\eg, interleaved executions) from code verification.

\ignore{
{\it Layering facilitates the invariant preservation proof.}
Invariant preservation proofs represent one of the most expensive
components of large-scale verification projects \cite{klein2009sel4}.
They are expensive because they need to be proved not only locally, but
also for the global execution of the whole program. Additionally, many low 
level data manipulation functions temporarily break invariants and reestablish them
later. In our layered approach, every global in-memory data
structure refines one or more isolated abstract logical states in
higher layers. This results in automatic isolation guarantees, despite the
fact that underlying implementations at lower layers still manipulate pointers to
the data structure. Furthermore, each abstraction layer may have a different set
of invariants in our framework, so invariants are introduced incrementally.
For example, consider a queue implemented as a doubly-linked list in
memory. If we directly impose the invariant that the queue is always
well-formed, then we need to prove that no other pointer manipulation in the kernel 
breaks this invariant, while also dealing with the situation where low level 
code temporarily breaks but then reestablishes the invariant. 
Instead, we first abstract
the concrete implementation of the queue into a simple logical list, hiding
concrete pointer manipulation beneath abstract read/write primitives.
Then at higher layers, we gradually introduce and verify the
abstract \texttt{enqueue} and \texttt{dequeue} operations that utilize these
read/write primitives. The well-formedness invariant is only introduced at this 
layer where the \texttt{enqueue} and \texttt{dequeue} operations are atomic and 
all the lower level primitives are already hidden. 
}


\ignore{
\newman{Don't read further. Below are just some random sentences got cut from the OSDI version.}

Furthermore, note that we did not reach the current working solution
in one shot. We first spent about 3 person months developing an
unsuccessful version of the framework for composing multi-threaded
execution on a single CPU.  In that version, thread-local execution
was modeled using a \emph{time stamp} index into a global system
log. We eventually realized that the exact time stamps were too
cumbersome and revealed too much information about the underlying
implementation (\eg, the number of software yields within a function
body), so we spent another month developing a new system that uses
local logs (lists of events) instead.\ignore{ of time stamps, and the
  ability to shuffle and merge the events in the local logs to hide
  unnecessary nondeterminism or implementation details.}  Our initial
multicore machine model also did not work out very well when we
developed the multicore linking framework; we spent 3 person weeks to
improve the initial design through multiple iterations.  The main
challenge was finding the right invariants for the environment
context, such that we could successfully establish starvation-freedom.

\paragraph{Abstraction Layers}
\newman{I am gonna write some summary about the layered approach after I read
the overview section to see how much of the concepts are already covered.}
\begin{itemize}
\item Abstraction of data representation: doubly-linked list --> logical list
\item Stronger invariants
\item Abstraction of primitive specification: hiding log implementation by linking and merging events
\item Refinement of machine models: from a realistic machine model to an ideal machine model that is suitable for our reasoning purpose, e.g., change in machine/memory/interrupt model
\end{itemize}

The verification effort roughly falls into three categories: layer
design with specification and invariants, refinement proofs between
the layers, and verification of C and assembly code with respect to
the specifications. The time needed for each of the categories depends
largely on the layer.  For instance, at the boundary of physical and
virtual memory management (\texttt{MPTIntro}), almost all effort
is in the refinement proof, due to the proof for the refinement between
two completely different memory models. More effort went into the
refinement proof when we introduced the Intel \emph{virtual machine
memory model}, where we proved the refinement between the concrete
four level extended page table structure in memory and the abstract
mapping from the guest addresses to the host addresses.
In contrast, for the layer \texttt{MATOp},
which initializes physical memory allocation,
most of the time was spent on verifying
the non-trivial nested loops present in the C code,
while the refinement proofs were derived automatically. 

The proofs were facilitated by automation tools for C
code, layer design patterns, and tactics libraries developed in
recent years \cite{dscal15}. These tools have greatly
reduced the amount of work needed to verify extensions of the kernel.


Verification not only
should not hinder application of similar performance optimizations,
but instead provide a safety net for more aggressive optimizations, if
it is required for application scenarios of the kernel we have in
mind.
}
